---
title: "Stekic et al. (2019)- Data Analysis"
output:   
  html_document:
    keep_md: true
    theme: journal
    toc: true
    toc_depth: 4
    toc_float: 
      collapsed: false
    code_folding: hide
---


#### Loading Libaries
```{r Loading Libraries, include = FALSE}
library(lme4)
library(plyr)
library(ggplot2)
library(afex)
library(emmeans)
library(ggthemes)
library(tidyverse)
library(kableExtra)
library(Hmisc)
library(binom)
library(Rmisc)
library(magick)
library(webshot)
library(magrittr)
library(multcomp)
```

```{r Loading my GGTheme, include = FALSE}
theme_alan <- function(base_size = 12 , base_family = "")
{
  half_line <- base_size/2
  colors <- ggthemes_data$few
  gray <- colors$medium["gray"]
  black <- colors$dark["black"]
  
  theme(
    line = element_line(colour = "black", size = 0.5, linetype = 1, lineend = "butt"),
    rect = element_rect(fill = "white", 
                        colour = "black", size = 0.5, linetype = 1),
    text = element_text(family = base_family, face = "plain", colour = "black", 
                        size = base_size, lineheight = 0.9, hjust = 0.5, vjust = 0.5,
                        angle = 0, margin = margin(), debug = FALSE),
    
    axis.line = element_blank(),
    axis.line.x = NULL,
    axis.line.y = NULL, 
    axis.text = element_text(size = rel(0.8), colour = "grey30"),
    axis.text.x = element_text(margin = margin(t = 0.8 * half_line/2), vjust = 1),
    axis.text.x.top = element_text(margin = margin(b = 0.8 * half_line/2), vjust = 0),
    axis.text.y = element_text(margin = margin(r = 0.8 * half_line/2), hjust = 1),
    axis.text.y.right = element_text(margin = margin(l = 0.8 * half_line/2), hjust = 0), 
    axis.ticks = element_line(colour = "grey20"), 
    axis.ticks.length = unit(half_line/2, "pt"),
    axis.title.x = element_text(margin = margin(t = half_line), vjust = 1),
    axis.title.x.top = element_text(margin = margin(b = half_line), vjust = 0),
    axis.title.y = element_text(angle = 90, margin = margin(r = half_line), vjust = 1),
    axis.title.y.right = element_text(angle = -90, margin = margin(l = half_line), vjust = 0),
    
    legend.background = element_rect(colour = NA),
    legend.spacing = unit(0.4, "cm"), 
    legend.spacing.x = NULL, 
    legend.spacing.y = NULL,
    legend.margin = margin(0.2, 0.2, 0.2, 0.2, "cm"),
    legend.key = element_rect(fill = "white", colour = NA), 
    legend.key.size = unit(1.2, "lines"), 
    legend.key.height = NULL,
    legend.key.width = NULL,
    legend.text = element_text(size = rel(0.8)), 
    legend.text.align = NULL,
    legend.title = element_text(hjust = 0),
    legend.title.align = NULL,
    legend.position = "right", 
    legend.direction = NULL,
    legend.justification = "center", 
    legend.box = NULL,
    legend.box.margin = margin(0, 0, 0, 0, "cm"),
    legend.box.background = element_blank(),
    legend.box.spacing = unit(0.4, "cm"),
    
    panel.background = element_rect(fill = "white", colour = NA),
    panel.border = element_rect(fill = NA, colour = "grey20"),
    panel.grid.major = element_line(colour = "grey92"),
    panel.grid.minor = element_line(colour = "grey92", size = 0.25),
    panel.spacing = unit(half_line, "pt"),
    panel.spacing.x = NULL,
    panel.spacing.y = NULL,
    panel.ontop = FALSE,
    
    strip.background = element_rect(fill = "NA", colour = "NA"),
    strip.text = element_text(colour = "grey10", size = rel(0.8)),
    strip.text.x = element_text(margin = margin(t = half_line, b = half_line)),
    strip.text.y = element_text(angle = 0, margin = margin(l = half_line, r = half_line)),
    strip.placement = "inside",
    strip.placement.x = NULL, 
    strip.placement.y = NULL,
    strip.switch.pad.grid = unit(0.1, "cm"), 
    strip.switch.pad.wrap = unit(0.1, "cm"), 
    
    plot.background = element_rect(colour = "white"),
    plot.title = element_text(size = rel(1.2), hjust = 0, vjust = 1, margin = margin(b = half_line * 1.2)),
    plot.subtitle = element_text(size = rel(0.9), hjust = 0, vjust = 1, margin = margin(b = half_line * 0.9)),
    plot.caption = element_text(size = rel(0.9), hjust = 1, vjust = 1, margin = margin(t = half_line * 0.9)), 
    plot.margin = margin(half_line, half_line, half_line, half_line),
    
    complete = TRUE)
}

```


# Reading in, Cleaning, and Preparing our data
```{r 1- Loading and Combining Data, message = FALSE, warning = FALSE}

#1 - Reading in the Raw Data
## Our data is in two formats because of a change in jsPsych version halfway through our data collection - thus we read those separate data file types (distinguishable by file size) in here separately, then combine them into a single large data frame

#1a - Specify our data folder for R
setwd("C:/Users/Alan Nielsen/Documents/GitHub/Stekic-et-al/Data/50s/") 



#1b - Take the names of every file in our directory
files50  <- list.files(pattern = '\\.csv')

#1c - Read each raw data file (called by name from read.csv) into a lists of lists (an indexed list)
tables50 <- lapply(files50, read.csv, header = TRUE) 

#1d- Bind the lists of lists into a data frame
combined.df.50 <- do.call(rbind , tables50) 

#1e- removes \ from all of the columns of the dataframe (These only appear in the .50 files from the jsPsych Update)
combined.df.50[] <- lapply(combined.df.50, function(x) gsub("\\\\", "", x)) 

#1f- Repeat steps a-d for second data folder (files output by original jsPsych before update)

setwd("C:/Users/Alan Nielsen/Documents/GitHub/Stekic-et-al/Data/40s/") 
files40  <- list.files(pattern = '\\.csv')
tables40 <- lapply(files40, read.csv, header = TRUE)
combined.df.40 <- do.call(rbind , tables40)

#1g -Make the column names of our two data frames the same so that they can be bound into a single data object
colnames(combined.df.50)<- c("rtD", "key_press", "trialtype2", "TrialIndex", "elapsed", "node_id", "viewhist", "responses", "Yoking", "RT", "RespKey", "RespCorr", "TrialType", "Image", "Label", "Location", "CorrectResponse", "Block", "BlockTrial", "Condition", "LabelType", "Subcondition", "TrialNum")


colnames(combined.df.40)<- c("rtD", "key_press", "trialtype2", "TrialIndex", "elapsed", "node_id", "viewhist", "responses", "Yoking", "RT", "RespKey", "RespCorr", "TrialType", "Image", "Label", "Location", "CorrectResponse", "Block", "BlockTrial", "Condition", "LabelType", "Subcondition", "TrialNum")
 
#1h - Binds our dataframes together 
combined.df <- rbind(combined.df.50, combined.df.40)

#1j- Substitute out some special characters
combined.df[] <- lapply(combined.df, function(x) gsub("\\\\", "", x))
combined.df[] <- lapply(combined.df, function(x) gsub("[{}]", "", x))
combined.df[] <- lapply(combined.df, function(x) gsub("\"", "", x))

```

```{r 2- Cleaning Data and setting the data up for analysis}

#2a- Add in a column with the biographical data (which is currently stored in a single value on the fourth line of each participant's file)
biodata <- combined.df[seq(4, nrow(combined.df), 246),]
biodata <- as.data.frame(biodata$responses)
colnames(biodata) <- "biodata"

biodata <- separate(biodata, col=biodata, into = c("Age", "Gender", "Specify"), sep = ",")

biodata$Age <- sub("age:", "", biodata$Age)
biodata$Gender <- sub("gender:", "", biodata$Gender)
biodata$Specify <- sub("specify:", "", biodata$Specify)

combined.df$Age <- rep(biodata$Age, each = 246)
combined.df$Gender <- rep(biodata$Gender, each = 246)

#2b- Add a unique participantID (actually the name of each file)
files <- c(files40, files50)
files <- sub(".csv", "", files)

combined.df$ParticipantID <- rep(files, each= 246)


#2c- Clean up our Data, Get rid of some useless columns, and re-sort the remaining columns into ones we will actually use
CleanData <- subset(combined.df, select = c("ParticipantID", "Age", "Gender", "Condition", "Subcondition", "Yoking", "TrialNum", "TrialType", "Block", "BlockTrial", "Image", "Label", "Location", "CorrectResponse", "RespKey", "RespCorr", "RT"))

#2d- Get rid of extra lines from the jsPsych output- leaving us with only our Trial data (everything else of use we've extracted and added as columns)
CleanData <- subset(CleanData, TrialNum > 0)

#2e- Set the data types of our various columns
CleanData$ParticipantID <- as.factor(CleanData$ParticipantID)
CleanData$Condition <- as.factor(CleanData$Condition)
CleanData$Subcondition <- as.factor(CleanData$Subcondition)
CleanData$Yoking <- as.factor(CleanData$Yoking)
CleanData$TrialType <- as.factor(CleanData$TrialType)
CleanData$Location <- as.factor(CleanData$Location)

CleanData$TrialNum <- as.numeric(CleanData$TrialNum)
CleanData$Block <- as.numeric(CleanData$Block)
CleanData$BlockTrial <- as.numeric(CleanData$BlockTrial)
CleanData$RespCorr <- as.numeric(CleanData$RespCorr)
CleanData$RT <- as.numeric(CleanData$RT)

```

```{r 3- Cleaning Data and setting the data up for analysis}

#3a- Clean up our Data, Get rid of some useless columns, and re-sort the remaining columns into ones we will actually use
CleanData <- subset(combined.df, select = c("ParticipantID", "Age", "Gender", "Condition", "Subcondition", "Yoking", "TrialNum", "TrialType", "Block", "BlockTrial", "Image", "Label", "Location", "CorrectResponse", "RespKey", "RespCorr", "RT"))

#3b- Get rid of extra lines from the jsPsych output- leaving us with only our Trial data (everything else of use we've extracted and added as columns)
CleanData <- subset(CleanData, TrialNum > 0)

#3c- Set the data types of our various columns
CleanData$ParticipantID <- as.factor(CleanData$ParticipantID)
CleanData$Condition <- as.factor(CleanData$Condition)
CleanData$Subcondition <- as.factor(CleanData$Subcondition)
CleanData$Yoking <- as.factor(CleanData$Yoking)
CleanData$TrialType <- as.factor(CleanData$TrialType)
CleanData$Location <- as.factor(CleanData$Location)

CleanData$TrialNum <- as.numeric(CleanData$TrialNum)
CleanData$Block <- as.numeric(CleanData$Block)
CleanData$BlockTrial <- as.numeric(CleanData$BlockTrial)
CleanData$RespCorr <- as.numeric(CleanData$RespCorr)
CleanData$RT <- as.numeric(CleanData$RT)


```

# Data Exclusion

We need to explore what data will be excluded from any further analysis off the top
Roughly we are looking for any irregularities in Reaction Time or other response data that we think would unduly affect our data

We remove data from participants for the following reasons:

A) `r nrow(CleanData[is.na(CleanData$RespCorr),])` participant (ID # `r CleanData[is.na(CleanData$RespCorr),]$ParticipantID`) for which there was an 'NA' value in one of their response columns (we cannot be certain of the source of this 'NA' value so we remove their data entirely to be safe)

B) `r ` participants (IDs = )


```{r 4- Data Exclusion I, warning = FALSE, message = FALSE}

#4A- Excluding any participants for which there are NA values recorded

#i- Look for any 'NA' values in our Response Correctness column
CleanData[is.na(CleanData$RespCorr),]

#ii- Remove participant for whom we have this impossible value
CleanData.RespCorr <- subset(CleanData, ParticipantID != "7gtriiTixvBaQ")

#4B- Excluding any participants with impossible RT values
## Preliminary inspection of data shows trials with negative response time values, which are obviously impossible
RTNegative <- subset(CleanData.RespCorr, RT < 0)

CleanData.RTNegCorr <- subset(CleanData.RespCorr, ParticipantID != "60CBPiTiO1gKw")
CleanData.RTNegCorr <- subset(CleanData.RTNegCorr, ParticipantID != "k3LHwiTiOx7fx")

# Verify that we have removed all negative values RT values from the data frame
RTNegative2 <- subset(CleanData.RTNegCorr, RT < 0)

#2B- Removing Participants with very large single RT values

RTHigh <- subset(CleanData.RespCorr, RT>120000 )

#Relevel this to get rid of factor levels that aren't there any longer
RTHigh$ParticipantID <- factor(RTHigh$ParticipantID)

RTHighs <- as.data.frame(table(RTHigh$ParticipantID))
colnames(RTHighs) <- c("Participant", "Count")

#re-order by count
RTHighs <- RTHighs[order(-RTHighs$Count),]

#Give shorter participantIDs
RTHighs$Participant <- substring(RTHighs$Participant, 1,3)

#Making the Participant Column into an Index
RTHighs2 <- RTHighs[-1]
row.names(RTHighs2) <- RTHighs$Participant

#Transpose for output
RTHighsT <- as.data.frame(t(RTHighs2))

#Get the participantIDs for participants with too-long RT values

participants <- unique(RTHigh$Participant)

#Write a for loop that removes all the lines of the data frame for each of these participants
CleanData.HighRTCorr <- CleanData.RTNegCorr

for (participant in participants) {
  
  CleanData.HighRTCorr <- subset(CleanData.HighRTCorr, ParticipantID != participant)
  
}

CleanData2 <- CleanData.HighRTCorr

#Output this clean Data to acsv
write.csv(CleanData2, file=("C:/Users/Alan Nielsen/Documents/GitHub/Stekic-et-al/Data/CleanData.csv"))

# RT Histograms

ggplot(CleanData2, aes(RT)) +
  geom_density() #+
  #xlim(0, 40000)

#Calculating the interquartile range

lowerquart <- quantile(CleanData2$RT)[2]
upperquart <- quantile(CleanData2$RT)[4]

Interquartile <- upperquart - lowerquart

#Calculating thresholds
#Mild thresholds are 1.5* interquartile range
mild.low <- lowerquart - (Interquartile * 1.5)
mild.high <- upperquart + (Interquartile * 1.5)

#Extremes are 3* interquartile range
extreme.low <- lowerquart - (Interquartile * 3)
extreme.high <- upperquart + (Interquartile * 3)


#1- Removing all Outliers ()
CleanData.RTTrim1 <- subset(CleanData2, RT > mild.low & RT < mild.high)

ggplot(CleanData.RTTrim1, aes(RT)) +
  geom_density() +
  ggtitle("Density Plot of RTs- All Outliers Removed")

#2- Replacing all Outliers with the Mean
CleanData.RTTrim2 <- CleanData2

#Compute non-outlier means
NOMean1 <- mean(CleanData.RTTrim1$RT)

CleanData.RTTrim2$RT <- ifelse(CleanData.RTTrim2$RT < mild.low,
                               NOMean1,
                               CleanData.RTTrim2$RT)

CleanData.RTTrim2$RT <- ifelse(CleanData.RTTrim2$RT > mild.high,
                               NOMean1,
                               CleanData.RTTrim2$RT)


ggplot(CleanData.RTTrim2, aes(RT)) +
  geom_density() +
  ggtitle("Density Plot of RTs- All Outliers Replaced with Mean")

#3- Removing only extreme outliers

CleanData.RTTrim3 <- subset(CleanData2, RT > extreme.low & RT < extreme.high)

ggplot(CleanData.RTTrim3, aes(RT)) +
  geom_density() +
  ggtitle("Density Plot of RTs- Extreme Outliers Removed")

#4- Setting outliers to the most extreme of the minimum outlier values

CleanData.RTTrim4 <- CleanData2

CleanData.RTTrim4$RT <- ifelse(CleanData.RTTrim4$RT < mild.low,
                               mild.low,
                               CleanData.RTTrim4$RT)

CleanData.RTTrim4$RT <- ifelse(CleanData.RTTrim4$RT > mild.high,
                               mild.high,
                               CleanData.RTTrim4$RT)

ggplot(CleanData.RTTrim4, aes(RT)) +
  geom_density() +
  ggtitle("Density Plot of RTs- Outliers Trimmed to Mild Outlier Boundary")

#5- Replacing all outliers with the mean on a by-subject basis

participantIDs <- unique(CleanData2$ParticipantID)

participantdata <- list()
CleanData.RTTrim5 <- list()

for (participant in participantIDs) {
  
  participantdata <- subset(CleanData2, ParticipantID == participant)
  
  #Get Non-Outlier Mean
  participantdataNO <- subset(participantdata, RT > mild.low & RT < mild.high)
  NOMean2 <- mean(participantdataNO$RT)
  
  

  participantdata$RT <- ifelse(participantdata$RT < mild.low,
                               NOMean2,
                               participantdata$RT)

  participantdata$RT <- ifelse(participantdata$RT > mild.high,
                                NOMean2,
                               participantdata$RT)
  

  
  CleanData.RTTrim5 <- rbind(CleanData.RTTrim5, participantdata)

}


ggplot(CleanData.RTTrim5, aes(RT)) +
  geom_density() +
  ggtitle("Density Plot of RTs- Outliers Replaced with Mean (By Subject)")

CleanData3 <- CleanData.RTTrim5




```

# Data Exclusion - Approachers and Spammers

```{r Data Exclusion - Approachers and Spammers, message = FALSE, warning = FALSE}

#Adding a column for recoding
CleanData2$holder <- paste(CleanData2$Location, CleanData2$RespKey, sep = "")

#Score a column by whether participants chose to approach or retreat
CleanData2$Approach <- mapvalues(CleanData2$holder,
                                 from = c("Left39", "Down38", "Up40", "Right37", 
                                          "Left37", "Down40", "Up38", "Right39"),
                                 to = c(rep("Approach", 4), rep("Retreat",4)
                                        )
                                 )


#First we will check for spammers - participants who vastly over-used any of the response keys

CleanData2$RespKey <- factor(CleanData2$RespKey)
KeyPresses <-
 CleanData2 %>%
 count(ParticipantID, RespKey, sort = FALSE)


## Determining our extreme outliers for a principled data exclusion policy
lowerquart2 <- quantile(KeyPresses$n)[2]
upperquart2 <- quantile(KeyPresses$n)[4]

Interquartile2 <- upperquart2 - lowerquart2

#Extremes are 3* interquartile range
extreme.low2 <- lowerquart2 - (Interquartile2 * 3)
extreme.high2 <- upperquart2 + (Interquartile2 * 3)

#Making a list of participants outside of the extreme range
SpammerKillList <- subset(KeyPresses, n > 88)
SpammerList <- unique(SpammerKillList$ParticipantID)

#Removing spammers from the data
CleanData5 <- CleanData2

for (Spammer in SpammerList) {
  
CleanData5 <- subset(CleanData5, ParticipantID != Spammer)
  
}

####################################################

#Now we will check for non-learners/non-listeners - i.e. those who always chose to approach or retreat

CleanData5$Approach <- factor(CleanData5$Approach)
Approachers <-
 CleanData5 %>%
 count(ParticipantID, Approach, sort = FALSE)

## Determining our extreme outliers for a principled data exclusion policy
lowerquart3 <- quantile(Approachers$n)[2]
upperquart3 <- quantile(Approachers$n)[4]

Interquartile3 <- upperquart3 - lowerquart3

#Extremes are 3* interquartile range
extreme.low3 <- lowerquart3 - (Interquartile3 * 3)
extreme.high3 <- upperquart3 + (Interquartile3 * 3)

#Making a list of participants outside of the extreme range
CowardsKillList <- subset(Approachers, n < 64)
CowardList <- unique(CowardsKillList$ParticipantID)

#Removing spammers from the data
CleanData6 <- CleanData5

for (Coward in CowardList) {
  
CleanData6 <- subset(CleanData6, ParticipantID != Coward)
  
}


```

We made the decision to exclude a number of participants we identified as "spammers" - those who drastically "preferred" pressing one of the arrow keys more than the others (note because of counterbalancing that a perfect experimental participant should use each response key equally). In the most flagrant case of this, one participant (ID = 'r KeyPresses$ParticipantID[[1]]` ) used the Left Arrow (Keypress 37) on 120 trials (every trial where it was a valid response), suggesting that they were simply pressing that key on all trials to advance through the experiment

We calculated 3x the Interquartile range as our extreme outlier cutoff point and excluded any participants who pressed a single response key 88 or more times

We take the same stance on Bravos/Cowards - those who either approach or retreat from Aliens at an extreme rate (again they should approach/retreat equally often)

# Data Exclusion - Response Times

So far we've been taking the approach of just trying to get our RT exclusion closer to normal, but we didn't really consider dropping any trials based on their RT values

I think after some further consideration we should consider doing so - see below

```{r Data Exclusion - Response Times}

#Above we looked only at the extremeness of Response TImes, but didn't consider the quality of the data from long trials. We certainly don't want to wholesale exclude data from entire participants for long trials, but it seems likely that in our extreme outliers we have individual trials where participants were likely to have been distracted by something else (I cannot imagine someone taking 15 seconds to truly consider a trial in this experiment)

#A first Look

extremeRTsHigh <- subset(CleanData6, RT > extreme.high) #Take only extremely slow trials
extremeRTsLow <- subset(CleanData6, RT < extreme.low) #Take only extremely fast trials (note that there actually aren't any extreme low RT trials)
normalTrials <- subset(CleanData6, RT < extreme.high) #Take only normal trials

highRTmeans <- data.frame(tapply(extremeRTsHigh$RespCorr, extremeRTsHigh$Condition, mean))
RTmeans <- data.frame(tapply(normalTrials$RespCorr, normalTrials$Condition, mean))

RTComparison <- cbind(highRTmeans, RTmeans)
RTComparison <- RTComparison[-1,] #remove a useless first row

RTComparison$Condition <- c(1, 10, 2, "3A", "3B", 4, 5, "6A", "6B", 7, 8, 9)
colnames(RTComparison) <- c("High RT Outliers", "Normal Trials", "Condition")

RTComparison <- subset(RTComparison, select = c("Condition", "Normal Trials", "High RT Outliers"))

RTComparison$Difference <- RTComparison$`Normal Trials` - RTComparison$`High RT Outliers`

RTComparison %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
            knitr::kable(caption = "Comparison of Proportion Correct on Normal Trials vs Extreme Outlier Trials (RT)", ) %>%
              kable_styling(full_width= F)



```

We can see in the above table that the differences in Proportion correct can be pretty substantial on very slow trials - up to 27% lower than on non-extreme trials, so we're going to simply remove these trials from the data entirely, rather than modifying the RT values for these trials to make our distribution for RT more normal

However the fairest way to do this is to actually calculate extreme values on a by trial-type basis and then exclude them, which we do below

```{r Data Exclusion - Response Times II}

#Training Trials
CleanData6.Training <- subset(CleanData6, TrialType == "Training")

CleanData6.Training.lowerquart <- quantile(CleanData6.Training$RT)[2]
CleanData6.Training.upperquart <- quantile(CleanData6.Training$RT)[4]

CleanData6.Training.Interquartile <- CleanData6.Training.upperquart - CleanData6.Training.lowerquart

CleanData6.Training.extreme.high <- CleanData6.Training.upperquart + (CleanData6.Training.Interquartile * 3)

CleanData6.Training.CutTrials <- subset(CleanData6.Training, RT > CleanData6.Training.extreme.high )
CleanData6.Training.CutTrials.ParticipantCounts <- count(CleanData6.Training.CutTrials, ParticipantID, sort = TRUE)

CleanData6.Training.UnCutTrials <- subset(CleanData6.Training, RT < CleanData6.Training.extreme.high )
 

#Testing Trials

CleanData6.Testing <- subset(CleanData6, TrialType == "Testing")

CleanData6.Testing.lowerquart <- quantile(CleanData6.Testing$RT)[2]
CleanData6.Testing.upperquart <- quantile(CleanData6.Testing$RT)[4]

CleanData6.Testing.Interquartile <- CleanData6.Testing.upperquart - CleanData6.Testing.lowerquart

CleanData6.Testing.extreme.high <- CleanData6.Testing.upperquart + (CleanData6.Testing.Interquartile * 3)

CleanData6.Testing.CutTrials <- subset(CleanData6.Testing, RT > CleanData6.Testing.extreme.high )
CleanData6.Testing.CutTrials.ParticipantCounts <- count(CleanData6.Testing.CutTrials, ParticipantID, sort = TRUE)

CleanData6.Testing.UnCutTrials <- subset(CleanData6.Testing, RT < CleanData6.Testing.extreme.high )


#Rebinding the Two Together
CleanData.RTTrimmed <- rbind(CleanData6.Training.UnCutTrials, CleanData6.Testing.UnCutTrials )

#Distribution of RTs after trimming extreme highs off
ggplot(CleanData.RTTrimmed, aes(RT)) +
  geom_density() +
  ggtitle("Density Plot of RTs- Extreme Outliers Removed")
```

```{r Data Cleaning - Adding Final Columns}

#Splitting Testing Trials
participantIDs <- unique(CleanData.RTTrimmed$ParticipantID)
participantdata <- list()
participantdata.training <- list()
participantdata.testing <- list()

testingtrials <- list()
trainingtrials <- list()


for (participant in participantIDs) {
  
  participantdata <- subset(CleanData3, ParticipantID == participant)
  participantdata.training <- subset(participantdata, TrialType == "Training")
  participantdata.testing <- subset(participantdata, TrialType == "Testing")
  
  
  TrainingFigures <- unique(participantdata.training$Image)
  
  participantdata.training$Generalisation <- NA
  
  participantdata.testing$Generalisation <- ifelse(
    participantdata.testing$Image %in% TrainingFigures,
    "Old",
    "New")
  
  trainingtrials <- rbind(trainingtrials, participantdata.training)
  testingtrials <- rbind(testingtrials, participantdata.testing)
  
  
}

CleanData4 <- rbind(trainingtrials, testingtrials)


#Factor levels, adding Trial Type 2

CleanData4$TrialType <- factor(CleanData4$TrialType, level = c ("Training", "Testing"))
CleanData4$TrialType2 <- paste(CleanData4$TrialType, CleanData4$Generalisation, sep = "-")
CleanData4$Generalisation <- as.factor(CleanData4$Generalisation)

CleanData4$TrialType2 <- factor(CleanData4$TrialType2, level = c ("Training-NA", "Testing-Old", "Testing-New"),
                                     labels = c("Training", "Testing-Old", "Testing-New"))


# Obtaining the curviness of images from our original script

CleanData4$Image <- sub("Stims/Figures/", "", CleanData4$Image)
CleanData4$Image <- sub(".bmp", "", CleanData4$Image)

CleanData.Final <- separate(CleanData4, col=Image, into = c("ImageSeed", "JaggedvsCurved", "Curviness", "Set"), sep = "-")

write.csv(CleanData.Final, file=("C:/Users/Alan Nielsen/Documents/GitHub/Stekic-et-al/Data/CleanDataFinal.csv"))


```


# The Thesis 

## Part I - Gary Lupyan and the **Cliff of Doom** (Lupyan Replication)

The first step with our data is to start with Lupyan et al. (2007) to see how well we replicate the basic data from that study

Our total experiment has many conditions, but only condition 3 (Conventional Category Label) and Condition 10 (No Label) are relevant for comparing to the data from Lupyan et al. (2007)

The first thing we need to do is take a look at our own counterbalanced subconditions (3A and 3B) to make sure that they can be collapsed for further data analysis

```{r Exploring and collapsing Condition 3 subconditions, warning = FALSE}

Exp3Sub <- subset(CleanData.Final, Condition =="3A"|Condition == "3B")

Exp3Sub$Condition <- factor(Exp3Sub$Condition)
Exp3Sub$Block <- factor(Exp3Sub$Block)

#Aggregating- Note this is only for the graph- we don't aggregate our data for analysis when using family = "binomial" - we model every single data point
Exp3SubAgg <- summarySE(Exp3Sub, measurevar = "RespCorr", groupvars = c("Condition", "TrialType2", "Block"))


afex.Exp3Sub <- mixed(RespCorr ~ Condition * TrialType2 + (1|ParticipantID),
                         data=Exp3Sub,
                         family=binomial,
                         control=glmerControl(optimizer="bobyqa"),
                         method = 'LRT',
                         progress=FALSE)


#Pretty up results and kick out as kable
afex.Exp3Sub$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Subcondition Comparison", ) %>%
              kable_styling(full_width= F)

pd <- position_dodge(width = 0.1)

ggplot(data=Exp3SubAgg, aes(x=Block, y=RespCorr, group= Condition)) +
  #geom_line(aes(color= Condition)) +
  #geom_point(size=1.75, aes(colour = Condition)) +
  geom_line(aes(linetype = Condition), size = 1.2, position=pd) +
  geom_errorbar(aes(ymin= RespCorr - ci, ymax= RespCorr + ci), width= 0.2, size = 1, position=pd)+
  scale_color_manual(values= c("#0066CC", "#CC0033","#33FF00", "#000000")) +
  labs(x="Block", y="Proportion of Correct Responses") +
  scale_y_continuous(limits = c(0.45,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  facet_grid(~TrialType2, scales="free", space= "free_x") +
  theme_alan() +
  ggtitle("Replication Performance comparing Subconditions of Condition 3")

#GGSAVE
ggsave("C:/Users/Alan Nielsen/Documents/GitHub/Stekic-et-al/Figures/Conventional Subconditions.png", plot = last_plot(), device = NULL, path = NULL,
  width = 8, height = 4, units = c("in", "cm", "mm"),
  dpi = 600)


#Participants per subcondition of condition 3
SubCounts3 <- data.frame(count(Exp3Sub, Condition))
SubCounts3$n <- SubCounts3$n/240
SubCounts3 %>% 
            knitr::kable(caption = "Number of Participants in Subconditions of Experiment 3", ) %>%
              kable_styling(full_width= F)

```

So above we see no significant effect of Condition when looking at only subconditions 3A and 3B, and no significant interaction of Condition x Trial type. Thus, as we expected, we are justified to collapse these subconditions for further analyses

```{r Importing and modifying data for Lupyan Replication Comparison}

# Subsetting out only the conditions for the replication
ReplicationData2007 <- subset(CleanData.Final, Condition == 10|Condition == "3A"|Condition == "3B")

# Name the levels of Condition, rather than having them be numbers
# This also collapses subconditions 3A and 3B (justified above)
ReplicationData2007$Condition <- factor(ReplicationData2007$Condition,
                            levels= c("3A", "3B", 10),
                            labels = c("Conventional Category","Conventional Category","No Label"))


#Aggregate our data for plotting
ReplicationAgg <- summarySE(ReplicationData2007, measurevar = "RespCorr", groupvars = c("Condition", "TrialType2", "Block"))

ReplicationAgg$Block <- factor(ReplicationAgg$Block)

#Plot our data
ggplot(data=ReplicationAgg, aes(x=Block, y=RespCorr, group= Condition)) +
  geom_line(aes(linetype = Condition), size = 1.2, position=pd) +
  geom_errorbar(aes(ymin= RespCorr - ci, ymax= RespCorr + ci), width= 0.2, size = 1, position=pd)+
  scale_color_manual(values= c("#0066CC", "#CC0033","#33FF00", "#000000")) +
  labs(x="Block", y="Proportion of Correct Responses") +
  scale_y_continuous(limits = c(0.4,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  facet_grid(~TrialType2, scales="free", space= "free_x") +
  theme_alan() +
  ggtitle("Replication of Lupyan (2017)")

#GGSAVE
ggsave("C:/Users/Alank/Documents/GitHub/Stekic-et-al/Figures/Replication 2007.png", plot = last_plot(), device = NULL, path = NULL,
  width = 8, height = 4, units = c("in", "cm", "mm"),
  dpi = 600)

```


We can also (of course) plot this alongside Gary's data to see how we stack up

```{r Plotting the Replication with the original data, warning= FALSE}

#Reading in Gary's Data
Lupyan2007 <- read.csv("C:/Users/Alank/Documents/GitHub/Stekic-et-al/RScripts/Lupyan2007Data.csv")
#adding columns we need (filled with NAs) to Gary's data (so the frames can be bound)
Lupyan2007$N <- NA
Lupyan2007$sd <- NA
Lupyan2007$se<- NA
Lupyan2007$ci <- NA

#Adding a "study" column to our data
ReplicationAgg2 <- summarySE(ReplicationData2007, measurevar = "RespCorr", groupvars = c("Condition", "TrialType", "Block"))
ReplicationAgg2$Study <- "Stekic et al (2020)"

#Combining the 2 dataframes into 1
ReplicationAgg2 <- rbind(ReplicationAgg2, Lupyan2007)

ReplicationAgg2$Group <- paste(ReplicationAgg2$Study, ReplicationAgg2$Condition)
ReplicationAgg2$Block <- factor(ReplicationAgg2$Block)

#Plotting the Data
ggplot(data=ReplicationAgg2, aes(x=Block, y=RespCorr, group= Group)) +
  geom_line(aes(linetype = Condition, color = Study), size = 1.2, position=pd) +
  geom_errorbar(aes(ymin= RespCorr - ci, ymax= RespCorr + ci, color = Study), width= 0.2, size = 1, position=pd)+
  scale_color_manual(values= c("#0066CC", "#CC0033","#33FF00", "#0066CC", "#CC0033","#33FF00", "#33FF00", "#000000")) +
  labs(x="Block", y="Proportion of Correct Responses") +
  scale_y_continuous(limits = c(0.45,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  facet_grid(~TrialType, scales="free", space= "free_x") +
  theme_alan()+
  ggtitle("Replication Performance split by Trial Type, comparing our data to Lupyan et al. (2007)")

ggsave("C:/Users/Alank/Documents/GitHub/Stekic-et-al/Figures/Replication 2007 (Including Lupyan).png", plot = last_plot(), device = NULL, path = NULL,
  width = 8, height = 4, units = c("in", "cm", "mm"),
  dpi = 600)


```


So you can see blah blah blah

Samesies things:

a) Training for Conventional Category label condition looks the same
b) Overall trend for No-Label conditions looks identical

Differencies things:
c) Lower performance on no label conditions across the board (why?)
d) TTHE CLIFF OF DOOOOOOOOOOOOOM

Of course we also need to verify what jumps out at us from the graphs statistically, so below is a statistical test for the replication (note we cannot directly compare our results to Lupyan (and to do so wouldn't make sense because different stims), but we can talk about what we find statistically relative to gary)

```{r Replication Statistics}

afex.Replication <- mixed(RespCorr ~ Condition * TrialType2 + (1|ParticipantID),
                         data=ReplicationData2007,
                         family=binomial,
                         control=glmerControl(optimizer="bobyqa"),
                         method = 'LRT')


#Pretty up results and kick out as kable
afex.Replication$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Mixed Model Results - Replication of Lupyan (2007)", ) %>%
              kable_styling(full_width= F)

##Fixed Effects 
summary(afex.Replication)$coefficients  %>%
  data.frame() %>%
    tibble::rownames_to_column(var= "FixedEffect") %>% 
    mutate(FixedEffect = recode_factor(FixedEffect, 
                                       `Condition1` = "No Label",
                                       `TrialType21` = "Testing - Old",
                                       `TrialType22` = "Testing - New",
                                       `Condition1:TrialType21` = "No Label x Testing - Old",
                                       `Condition1:TrialType22` = "No Label x Testing - New"
                                       )) %>%
      `colnames<-`(c("Fixed Effect", "Estimate", "SE", "z", "p")) %>%
        mutate_if(is.numeric, round, digits = 3) %>% 
          mutate_at("p", funs(ifelse(.<0.001, "<0.001", .))) %>%
            knitr::kable(caption = "Word Type Comparison- Fixed Effects Table", ) %>%
              kable_styling(full_width= F) %>%
                pack_rows("Intercept", 1, 1) %>%
                  pack_rows("Condition", 2, 2) %>%
                    pack_rows("Trial Type", 3, 4)%>%
                      pack_rows("Interaction", 5, 6)



```

So here we see what I think look like sensible results - there is no main effect of Condition, because it is subsumed by the Interaction effect (which is where the money is) - Participants who learn a Conventional Category label do better on Training Trials, but worse on Testing Trials (of both types)

## RT Analyses of Replication

```{r RT Analysis for Replication}


afex.Replication.RT <- mixed(RT ~ Condition * TrialType2 + (1|ParticipantID),
                         data=ReplicationData2007,
                         method = 'LRT')


#Pretty up results and kick out as kable
afex.Replication.RT$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Mixed Model Results - Replication of Lupyan (2007)", ) %>%
              kable_styling(full_width= F)

##Fixed Effects 
summary(afex.Replication.RT)$coefficients  %>%
  data.frame() %>%
    tibble::rownames_to_column(var= "FixedEffect") %>% 
    mutate(FixedEffect = recode_factor(FixedEffect, 
                                       `Condition1` = "No Label",
                                       `TrialType21` = "Testing - Old",
                                       `TrialType22` = "Testing - New",
                                       `Condition1:TrialType21` = "No Label x Testing - Old",
                                       `Condition1:TrialType22` = "No Label x Testing - New"
                                       )) %>%
      `colnames<-`(c("Fixed Effect", "Estimate", "SE", "df", "t", "p")) %>%
        mutate_if(is.numeric, round, digits = 3) %>% 
          mutate_at("p", funs(ifelse(.<0.001, "<0.001", .))) %>%
            knitr::kable(caption = "Word Type Comparison- Fixed Effects Table", ) %>%
              kable_styling(full_width= F) %>%
                pack_rows("Intercept", 1, 1) %>%
                  pack_rows("Condition", 2, 2) %>%
                    pack_rows("Trial Type", 3, 4)%>%
                      pack_rows("Interaction", 5, 6)


ReplicationAggRT <- summarySE(ReplicationData2007, measurevar = "RT", groupvars = c("Condition", "TrialType2", "Block"))

ReplicationAggRT$Block <- factor(ReplicationAggRT$Block)

#Plot our data
ggplot(data=ReplicationAggRT, aes(x=Block, y=RT, group= Condition)) +
  geom_line(aes(linetype = Condition), size = 1.2, position=pd) +
  geom_errorbar(aes(ymin= RT - ci, ymax= RT + ci), width= 0.2, size = 1, position=pd)+
  scale_color_manual(values= c("#0066CC", "#CC0033","#33FF00", "#000000")) +
  labs(x="Block", y="Response Time (ms)") +
  #scale_y_continuous(limits = c(0.45,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  facet_grid(~TrialType2, scales="free", space= "free_x") +
  theme_alan() +
  ggtitle("Replication of Lupyan (2017)- Response Times")

#GGSAVE
ggsave("C:/Users/Alank/Documents/GitHub/Stekic-et-al/Figures/Replication 2007 - Response Times.png", plot = last_plot(), device = NULL, path = NULL,
  width = 8, height = 4, units = c("in", "cm", "mm"),
  dpi = 600)


```

The other thing to look at is the learning trajectory for training (i.e. the effect of Block)
Previous models were not converging and this I think had to do with "double dipping" the "block" values- i.e. mostly a naming thing



```{r Block Analysis for training}

ReplicationData2007$Block <- factor(ReplicationData2007$Block)
afex.Replication.Block <- mixed(RespCorr ~ Condition * Block + (1|ParticipantID),
                         data=subset(ReplicationData2007, TrialType2 == "Training") ,
                         family=binomial,
                         control=glmerControl(optimizer="bobyqa"),
                        method = 'LRT')

afex.Replication.Block$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Mixed Model Results - Replication of Lupyan (2007) - Effect of Block", ) %>%
              kable_styling(full_width= F)


#Learning Trajectories (effect of block)
##Block Descriptives
Replication.Block <- summarySE(data=subset(ReplicationData2007, TrialType2 == "Training"),
                             measurevar = "RespCorr",
                             groupvars = "Block")

##Blocks vs. Chance
Replication.Block.vchance <- test(emmeans(afex.Replication.Block, 
                    ~as.factor(Block),
                    adjust = "none"))

##Blocks vs. Each Other

Replication.Block.vPairs <- data.frame(pairs(emmeans(afex.Replication.Block, 
                    ~Block,
                    adjust = "none")))

###Contrasts of Interest
Contrasts.firstorder <- c("1 - 2", "2 - 3", "3 - 4", "4 - 5", "5 - 6", "6 - 7", "7 - 8", "8 - 9")
Contrasts.firstorder.vals <- filter(Replication.Block.vPairs, contrast %in% Contrasts.firstorder)
Contrasts.firstorder.vals <- rbind(rep(NA, 6), Contrasts.firstorder.vals) #Put in a dummy first row

Contrasts.secondorder <- c("1 - 3", "2 - 4", "3 - 5", "4 - 6", "5 - 7", "6 - 8", "7 - 9")
Contrasts.secondorder.vals <- filter(Replication.Block.vPairs, contrast %in% Contrasts.secondorder)
Contrasts.secondorder.vals <- rbind(rep(NA, 6), rep(NA, 6), Contrasts.secondorder.vals) #Put in a dummy first two rows



###Creating a single dataframe for Blocks
Replication.Block.Combo <- cbind.data.frame (
  Replication.Block$Block, Replication.Block$RespCorr, 
 Replication.Block.vchance$z.ratio,Replication.Block.vchance$p.value,
Contrasts.firstorder.vals$z.ratio, Contrasts.firstorder.vals$p.value,
Contrasts.secondorder.vals$z.ratio, Contrasts.secondorder.vals$p.value
)


###Knitting Block Table
Replication.Block.Combo %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
          `colnames<-`(c("Block", "Mean",  
                                "z value", "p value",
                         "z value ", "p value ",
                         "z value  ", "p value  "
                         )) %>%
              mutate_at(.vars= vars("p value", "p value ", "p value  "), funs(ifelse(.<0.001, "<0.001", .))) %>%
                knitr::kable(caption = "Replication - Learning Trajectory (Effect of Block)","html") %>%
                  kable_styling(full_width= F) %>%
                add_header_above(c("", "",  "Comparison to Chance" = 2, 
                               "Comparison to Previous Block" = 2,
                                "Comparison to Two Blocks Before" = 2),
                               font_size = 12)

                

                            
                            
```

So looking at the data in this way isn't ultimately as interesting as I had hoped it would be- the post hoc comparisons at least

I think the basic Linear Model Outputs are actually all that we need, but they are maybe a little bit difficult to really describe - and here we actually have both main effects *and* an interaction. The interaction tells us that performance does not increase over blocks in the same way for both conditions of the experiment, but i'm not certain that this is meaningful - they do seem to follow pretty similar trajectories

SO I"M NOT SURE WHAT TO MAKE OF THAT

# LUPYAN & CASASANTO EXTENSION - Exploring the effect of Iconicity on Category Label Learning

```{r Visualising Part II}

# Subsetting out only the conditions for the replication
Part2Data <- subset(CleanData.Final, Condition == 10|Condition == "3A"|Condition == "3B"|Condition == 1|Condition == 2)

# Name the levels of Condition, rather than having them be numbers
# This also collapses subconditions 3A and 3B (justified above)
Part2Data$Condition <- factor(Part2Data$Condition,
                            levels= c(10, 1, 2, "3A", "3B"),
                            labels = c("No Label", "Iconic (Category)", "Counter-Iconic (Category)", "Conventional (Category)",
                                       "Conventional (Category)"))

#Aggregate our data for plotting
Part2Agg <- summarySE(Part2Data, measurevar = "RespCorr", groupvars = c("Condition", "TrialType2", "Block"))
Part2Agg$Block <- factor(Part2Agg$Block)

#Plot our data
ggplot(data=Part2Agg, aes(x=Block, y=RespCorr, group= Condition)) +
  geom_line(aes(linetype = Condition, color = Condition), size = 1.2, position=pd) +
  geom_errorbar(aes(ymin= RespCorr - ci, ymax= RespCorr + ci, color= Condition), width= 0.2, size = 1, position=pd)+
  scale_color_manual(values= c("#000000", "#007e70","#00b19d", "#00e4ca")) +
  scale_linetype_manual(values = c("dashed", rep("solid",3))) +
  labs(x="Block", y="Proportion of Correct Responses") +
  scale_y_continuous(limits = c(0.40,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  facet_grid(~TrialType2, scales="free", space= "free_x") +
  theme_alan() +
  ggtitle("Category Label Comparison")

#GGSAVE
ggsave("C:/Users/Alank/Documents/GitHub/Stekic-et-al/Figures/Category Labels.png", plot = last_plot(), device = NULL, path = NULL,
  width = 8, height = 4, units = c("in", "cm", "mm"),
  dpi = 600)

```

```{r Part 2 Statistics - Main Effects}

afex.Part2 <- mixed(RespCorr ~ Condition * TrialType2 + (1|ParticipantID),
                         data=Part2Data,
                         family=binomial,
                         control=glmerControl(optimizer="bobyqa"),
                         method = 'LRT')


#Pretty up results and kick out as kable
afex.Part2$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Mixed Model Results - Part 2", ) %>%
              kable_styling(full_width= F)

##Fixed Effects 
summary(afex.Part2)$coefficients  %>%
  data.frame() %>%
    tibble::rownames_to_column(var= "FixedEffect") %>% 
    mutate(FixedEffect = recode_factor(FixedEffect, 
                                       `Condition1` = "Iconic",
                                        `Condition2` = "Counter-Iconic",
                                       `Condition3` = "Conventional",
                                       `TrialType21` = "Testing - Old",
                                       `TrialType22` = "Testing - New",
                                       `Condition1:TrialType21` = "Iconic x Testing - Old",
                                       `Condition2:TrialType21` = "Counter-Iconic x Testing - Old",
                                       `Condition3:TrialType21` = "Conventional x Testing - Old",
                                       `Condition1:TrialType22` = "Iconic x Testing - New",
                                       `Condition2:TrialType22` = "Counter-Iconic x Testing - New",
                                       `Condition3:TrialType22` = "Conventional x Testing - New"
                                       )) %>%
      `colnames<-`(c("Fixed Effect", "Estimate", "SE", "z", "p")) %>%
        mutate_if(is.numeric, round, digits = 3) %>% 
          mutate_at("p", funs(ifelse(.<0.001, "<0.001", .))) %>%
            knitr::kable(caption = "Word Type Comparison- Fixed Effects Table", ) %>%
              kable_styling(full_width= F) %>%
                pack_rows("Intercept", 1, 1) %>%
                  pack_rows("Condition", 2, 4) %>%
                    pack_rows("Trial Type", 5, 6)%>%
                      pack_rows("Condition x Trial Type Interactions", 7, 12)



##Post-Hoc Effect of Condition
Part2.Condition <- summarySE(data=Part2Data,
                             measurevar = "RespCorr",
                             groupvars = "Condition")

Part2.Condition.EM <- emmeans(afex.Part2, ~Condition)

##Condition vs. Chance
Part2.Condition.vchance <- test(Part2.Condition.EM, adjust = "none")

##Condition vs. Each Other
Part2.Condition.vPairs <- data.frame(pairs(Part2.Condition.EM, adjust = "none" ))

###NoLabel Baseline
Part2.Condition.vPairs.1 <- 
  rbind(rep(NA,6),Part2.Condition.vPairs[1:3,])
#Iconic Baseline
Part2.Condition.vPairs.2 <- 
  rbind(Part2.Condition.vPairs[1,], rep(NA,6),Part2.Condition.vPairs[4:5,])
#Counter-Iconic Baseline
Part2.Condition.vPairs.3 <- 
  rbind(Part2.Condition.vPairs[2,], Part2.Condition.vPairs[4,], rep(NA,6),Part2.Condition.vPairs[6,])
#Conventional Baseline
Part2.Condition.vPairs.4 <- 
  rbind(Part2.Condition.vPairs[3,],Part2.Condition.vPairs[5,],Part2.Condition.vPairs[6,], rep(NA,6))

###Creating a single dataframe for Blocks
Part2.Condition.Combo <- cbind.data.frame (
  Part2.Condition$Condition, Part2.Condition$N, Part2.Condition$RespCorr, Part2.Condition$se,
  Part2.Condition.vchance$z.ratio,Part2.Condition.vchance$p.value,
  Part2.Condition.vPairs.1$p.value,
  Part2.Condition.vPairs.2$p.value,
  Part2.Condition.vPairs.3$p.value,
  Part2.Condition.vPairs.4$p.value
)

colnames(Part2.Condition.Combo) <- c("Condition", "n", "Mean", "SE", 
                                "z v chance", "p v chance",
                                "p v NL",
                                "p v I",
                                "p v CI",
                                "p v Con"
                                )

###Knitting Condition Table
Part2.Condition.Combo %>%  
  mutate_if(is.numeric, round, 3) %>%
    mutate_at(.vars = "z v chance", round, 2)%>%
    mutate_at(.vars = vars("p v chance", "p v NL", "p v I", "p v CI", "p v Con"),
        funs(ifelse(.<0.001, "<0.001", .))) %>%
          knitr::kable(caption = 'Part II- Descriptive Statistics - Condition', "html") %>%
            kable_styling(full_width = F) %>%
              add_header_above(c("", "Descriptive Stats" = 3, "Comparison to Chance" = 2, 
                     "vs No Label" = 1,
                     "vs Iconic" = 1,
                     "vs Counter-Iconic" = 1,
                     "vs Conventional" = 1),
                   font_size = 12)

##################################################################################


```

We now need post-hoc tests to better pull apart these effects. 

For post hoc tests obviously we don't see anything looking at just a Condition effect (because there isn't one)
What you need are post hoc tests of the interaction effects

```{r Part 2- Post HOc Interactions}

Part2.Interaction.vPairs <- data.frame(pairs(emmeans(afex.Part2, 
                    ~Condition * TrialType2,
                    adjust = "none")))

#TRAINING 
Part2.Interaction.vPairs.Training.1 <-   rbind(rep(NA,6),Part2.Interaction.vPairs[1:3,])
Part2.Interaction.vPairs.Training.2 <-   rbind(Part2.Interaction.vPairs[1,], rep(NA, 6), Part2.Interaction.vPairs[12:13,])
Part2.Interaction.vPairs.Training.3 <-   rbind(Part2.Interaction.vPairs[2,], Part2.Interaction.vPairs[12,], rep(NA, 6), Part2.Interaction.vPairs[22,])
Part2.Interaction.vPairs.Training.4 <-   rbind(Part2.Interaction.vPairs[3,], Part2.Interaction.vPairs[13,], Part2.Interaction.vPairs[22,], rep(NA, 6))

#TESTING OLD
Part2.Interaction.vPairs.TestingOld.1 <-   rbind(rep(NA,6),Part2.Interaction.vPairs[39:41,])
Part2.Interaction.vPairs.TestingOld.2 <-   rbind(Part2.Interaction.vPairs[39,], rep(NA, 6), Part2.Interaction.vPairs[46:47,])
Part2.Interaction.vPairs.TestingOld.3 <-   rbind(Part2.Interaction.vPairs[40,], Part2.Interaction.vPairs[46,], rep(NA, 6),
                                                 Part2.Interaction.vPairs[52,])
Part2.Interaction.vPairs.TestingOld.4 <-   rbind(Part2.Interaction.vPairs[41,], Part2.Interaction.vPairs[47,], Part2.Interaction.vPairs[52,],
                                                 rep(NA, 6))

#TESTING NEW
Part2.Interaction.vPairs.TestingNew.1 <-   rbind(rep(NA,6),Part2.Interaction.vPairs[61:63,])
Part2.Interaction.vPairs.TestingNew.2 <-   rbind(Part2.Interaction.vPairs[61,], rep(NA, 6), Part2.Interaction.vPairs[64:65,])
Part2.Interaction.vPairs.TestingNew.3 <-   rbind(Part2.Interaction.vPairs[62,], Part2.Interaction.vPairs[64,], rep(NA, 6),
                                                 Part2.Interaction.vPairs[66,])
Part2.Interaction.vPairs.TestingNew.4 <-   rbind(Part2.Interaction.vPairs[63,], Part2.Interaction.vPairs[65,], Part2.Interaction.vPairs[66,],
                                                 rep(NA, 6))

# I CAN'T FEEL FEELINGS ANYMORE
Part2.Interaction.Combo <- cbind.data.frame (Part2.Condition.Combo, 
  Part2.Interaction.vPairs.Training.1$p.value,
  Part2.Interaction.vPairs.Training.2$p.value,
  Part2.Interaction.vPairs.Training.3$p.value,
  Part2.Interaction.vPairs.Training.4$p.value,
  Part2.Interaction.vPairs.TestingOld.1$p.value,
  Part2.Interaction.vPairs.TestingOld.2$p.value,
  Part2.Interaction.vPairs.TestingOld.3$p.value,
  Part2.Interaction.vPairs.TestingOld.4$p.value,
  Part2.Interaction.vPairs.TestingNew.1$p.value,
  Part2.Interaction.vPairs.TestingNew.2$p.value,
  Part2.Interaction.vPairs.TestingNew.3$p.value,
  Part2.Interaction.vPairs.TestingNew.4$p.value
)

colnames(Part2.Interaction.Combo) <- c("Condition", "n", "Mean", "SE", 
                                "z v chance", "p v chance",
                                "NL (M)", "I (M)", "CI (M)", "Con (M)",
                                "NL (T)", "I (T)", "CI (T)", "Con (T)",
                                "NL (O)", "I (O)", "CI (O)", "Con (O)",
                                "NL (N)", "I (N)", "CI (N)", "Con (N)"
                                )

Part2.Interaction.Combo %>%  
    mutate_if(is.numeric, round, 3) %>%
    mutate_at(.vars = "z v chance", round, 2)%>%
    mutate_at(.vars = vars("p v chance",
                                "NL (M)", "I (M)", "CI (M)", "Con (M)",
                                "NL (T)", "I (T)", "CI (T)", "Con (T)",
                                "NL (O)", "I (O)", "CI (O)", "Con (O)",
                                "NL (N)", "I (N)", "CI (N)", "Con (N)"),
        funs(ifelse(.<0.001, "<0.001", .))) %>%
          knitr::kable(caption = 'Part II - Post Hoc Tests - Interactions of Condition x Trial Type', "html") %>%
            kable_styling(full_width = F) %>%
              add_header_above(c("", "Descriptive Stats" = 3, "Comparison to Chance" = 2, 
                     "vs No Label" = 1, "vs Iconic" = 1, "vs Counter-Iconic" = 1, "vs Conventional" = 1,
                      "vs No Label" = 1, "vs Iconic" = 1, "vs Counter-Iconic" = 1, "vs Conventional" = 1,
                     "vs No Label" = 1, "vs Iconic" = 1, "vs Counter-Iconic" = 1, "vs Conventional" = 1,
                     "vs No Label" = 1, "vs Iconic" = 1, "vs Counter-Iconic" = 1, "vs Conventional" = 1),
                   font_size = 12)  %>%
                add_header_above(c(" " = 6,  "Main Effect" = 4, "Training Trials" = 4, "Testing - Old Trials" = 4, "Testing - New Trials" = 4), 
                                 font_size = 14)




```


So unfortunately it can't kick that all out as a single viewable table (it's too big - glad I wasted an hour on it!)

But we can kick out parts as individual tables

```{r Part II Post Hoc Tests}

#COMPARISONS OF CONDITIONS WITHIN TRAINING TRIALS
subset(Part2.Interaction.Combo, select = c(Condition, Mean, `NL (T)`, `I (T)`, `CI (T)`, `Con (T)`)) %>%  
    mutate_if(is.numeric, round, 3) %>%
    mutate_at(.vars = vars("NL (T)", "I (T)", "CI (T)", "Con (T)"),
        funs(ifelse(.<0.001, "<0.001", .))) %>%
          knitr::kable(caption = 'Part II - Post Hoc Comparisons of Condition within Training Trials', "html") %>%
            kable_styling(full_width = F) %>%
              add_header_above(c(" " = 2, "Post Hoc Comparisons (p values)" = 4),
                   font_size = 12) 

#COMPARISONS OF CONDITIONS WITHIN TESTING TRIALS
subset(Part2.Interaction.Combo, select = c(Condition, Mean, `NL (O)`, `I (O)`, `CI (O)`, `Con (O)`,
                                           `NL (N)`, `I (N)`, `CI (N)`, `Con (N)`)) %>%  
    mutate_if(is.numeric, round, 3) %>%
    mutate_at(.vars = vars(`NL (O)`, `I (O)`, `CI (O)`, `Con (O)`, `NL (N)`, `I (N)`, `CI (N)`, `Con (N)`),
        funs(ifelse(.<0.001, "<0.001", .))) %>%
          knitr::kable(caption = 'Part II - Post Hoc Comparisons of Condition within Testing Trials', "html", booktabs = T) %>%
            kable_styling(full_width = F) %>%
              add_header_above(c(" " = 2, "Testing - Old Trials" = 4, "Testing - New Trials" = 4),
                   font_size = 12) %>%
                add_header_above(c(" " = 2,  "Post Hoc Comparisons (p values)" = 8), 
                                 font_size = 14)


```
Finally something that gives a nice look.

We can see that looking within Training Trials all of the conditions are different from No Label (but not different from each other)

Within the Testing Trials however, none of the conditions are significantly different from each other - this seems a bit surprising but the variance within conditions is larger in Testing Trials and the number of trials fewer


```{r RT Analysis for PART II}
afex.Part2.RT <- mixed(RT ~ Condition * TrialType2 + (1|ParticipantID),
                         data=Part2Data,
                         method = 'LRT')


#Pretty up results and kick out as kable
afex.Part2.RT$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Mixed Model Results - RTs - Part II", ) %>%
              kable_styling(full_width= F)

##Fixed Effects 
summary(afex.Part2.RT)$coefficients  %>%
  data.frame() %>%
    tibble::rownames_to_column(var= "FixedEffect") %>% 
    mutate(FixedEffect = recode_factor(FixedEffect, 
                                       `Condition1` = "Iconic",
                                        `Condition2` = "Counter-Iconic",
                                       `Condition3` = "Conventional",
                                       `TrialType21` = "Testing - Old",
                                       `TrialType22` = "Testing - New",
                                       `Condition1:TrialType21` = "Iconic x Testing - Old",
                                       `Condition2:TrialType21` = "Counter-Iconic x Testing - Old",
                                       `Condition3:TrialType21` = "Conventional x Testing - Old",
                                       `Condition1:TrialType22` = "Iconic x Testing - New",
                                       `Condition2:TrialType22` = "Counter-Iconic x Testing - New",
                                       `Condition3:TrialType22` = "Conventional x Testing - New"
                                       )) %>%
      `colnames<-`(c("Fixed Effect", "Estimate", "SE", "df", "t", "p")) %>%
        mutate_if(is.numeric, round, digits = 3) %>% 
          mutate_at("p", funs(ifelse(.<0.001, "<0.001", .))) %>%
            knitr::kable(caption = "Word Type Comparison- Fixed Effects Table - RTs - Part 2", ) %>%
              kable_styling(full_width= F) %>%
                pack_rows("Intercept", 1, 1) %>%
                  pack_rows("Condition", 2, 4) %>%
                    pack_rows("Trial Type", 5, 6)%>%
                      pack_rows("Condition x Trial Type Interactions", 7, 12)


Part2AggRT <- summarySE(Part2Data, measurevar = "RT", groupvars = c("Condition", "TrialType2", "Block"))

Part2AggRT$Block <- factor(Part2AggRT$Block)

#Plot our data
ggplot(data=Part2AggRT, aes(x=Block, y=RT, group= Condition)) +
  geom_line(aes(linetype = Condition, color = Condition), size = 1.2, position=pd) +
  geom_errorbar(aes(ymin= RT - ci, ymax= RT + ci, color= Condition), width= 0.2, size = 1, position=pd)+
  scale_color_manual(values= c("#000000", "#007e70","#00b19d", "#00e4ca")) +
  scale_linetype_manual(values = c("dashed", rep("solid",3))) +
  labs(x="Block", y="Response Time") +
  #scale_y_continuous(limits = c(0.40,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  facet_grid(~TrialType2, scales="free", space= "free_x") +
  theme_alan() +
  ggtitle("Response Time Comparison - Part 2")

#GGSAVE
ggsave("C:/Users/Alank/Documents/GitHub/Stekic-et-al/Figures/Response Time - Part 2.png", plot = last_plot(), device = NULL, path = NULL,
  width = 8, height = 4, units = c("in", "cm", "mm"),
  dpi = 600)


```

There are no RT effects at all, so we don't need to even think about doing post hoc tests


```{r Part 2 - Block Analysis for Training Trials}

afex.Part2.Block <- mixed(RespCorr ~ Condition * Block + (1|ParticipantID),
                         data=subset(Part2Data, TrialType2 == "Training") ,
                        method = 'LRT', family = binomial)

afex.Part2.Block$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Mixed Model Results - Extension of Lupyan & Cassasanto (2014) - Effect of Block", ) %>%
              kable_styling(full_width= F)


```
Again these block effects are very hard to sensibly interpret


# PART 3 - INDIVIDUAL LABELS

To take a look at individual labels we first need to justify the collapsing of subconditions

In this case we want to look at differences between 6A and 6B and between 7, 8 , and 9 (our "arbitrary" conditions)

```{r Exploring and collapsing Condition 6 subconditions, warning = FALSE}

Exp6Sub <- subset(CleanData.Final, Condition =="6A"|Condition == "6B")

Exp6Sub$Condition <- factor(Exp6Sub$Condition)
Exp6Sub$Block <- factor(Exp6Sub$Block)

#Aggregating- Note this is only for the graph- we don't aggregate our data for analysis when using family = "binomial" - we model every single data point
Exp6SubAgg <- summarySE(Exp6Sub, measurevar = "RespCorr", groupvars = c("Condition", "TrialType2", "Block"))


afex.Exp6Sub <- mixed(RespCorr ~ Condition * TrialType2 + (1|ParticipantID),
                         data=Exp6Sub,
                         family=binomial,
                         control=glmerControl(optimizer="bobyqa"),
                         method = 'LRT',
                         progress=FALSE)


#Pretty up results and kick out as kable
afex.Exp6Sub$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Subcondition Comparison (Condition 6A and 6B)", ) %>%
              kable_styling(full_width= F)

pd <- position_dodge(width = 0.1)

ggplot(data=Exp6SubAgg, aes(x=Block, y=RespCorr, group= Condition)) +
  #geom_line(aes(color= Condition)) +
  #geom_point(size=1.75, aes(colour = Condition)) +
  geom_line(aes(linetype = Condition), size = 1.2, position=pd) +
  geom_errorbar(aes(ymin= RespCorr - ci, ymax= RespCorr + ci), width= 0.2, size = 1, position=pd)+
  scale_color_manual(values= c("#0066CC", "#CC0033","#33FF00", "#000000")) +
  labs(x="Block", y="Proportion of Correct Responses") +
  scale_y_continuous(limits = c(0.45,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  facet_grid(~TrialType2, scales="free", space= "free_x") +
  theme_alan() +
  ggtitle("Comparing Subconditions of Condition 6")

#GGSAVE
ggsave("C:/Users/Alank/Documents/GitHub/Stekic-et-al/Figures/Conventional Subconditions - 6.png", plot = last_plot(), device = NULL, path = NULL,
  width = 8, height = 4, units = c("in", "cm", "mm"),
  dpi = 600)


#SUBCONDITION COUNTS
SubCounts6 <- data.frame(count(Exp6Sub, Condition))
SubCounts6$n <- SubCounts6$n/240
SubCounts6 %>% 
            knitr::kable(caption = "Number of Participants in Subconditions of Experiment 6", ) %>%
              kable_styling(full_width= F)

```

We have the same condition here as with Subconditions for Condition 3. There is no effect during Training but for whatever reason during Testing Trials one of the mappings is **much** better than the other. It's very hard to conceptualize why this would be the case. It *may* suggest that one of the mappings is better than the others - this probably bears mentioning but given that we spend very little time actually discussing the effects on Testing Trials (because of the cliff of doom) I'm happy to collapse these

```{r Exploring and collapsing Arbitrary Conditions (7,8,9), warning = FALSE}

ArbSub <- subset(CleanData.Final, Condition ==7|Condition == 8|Condition == 9)

ArbSub$Condition <- factor(ArbSub$Condition)
ArbSub$Block <- factor(ArbSub$Block)

#Aggregating- Note this is only for the graph- we don't aggregate our data for analysis when using family = "binomial" - we model every single data point
ArbSubAgg <- summarySE(ArbSub, measurevar = "RespCorr", groupvars = c("Condition", "TrialType2", "Block"))


afex.ArgSub <- mixed(RespCorr ~ Condition * TrialType2 + (1|ParticipantID),
                         data=ArbSub,
                         family=binomial,
                         control=glmerControl(optimizer="bobyqa"),
                         method = 'LRT',
                         progress=FALSE)


#Pretty up results and kick out as kable
afex.ArgSub$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Arbitrary Condition Comparison (Conditions 7,8,9)", ) %>%
              kable_styling(full_width= F)

pd <- position_dodge(width = 0.1)

ggplot(data=ArbSubAgg, aes(x=Block, y=RespCorr, group= Condition)) +
  #geom_line(aes(color= Condition)) +
  #geom_point(size=1.75, aes(colour = Condition)) +
  geom_line(aes(linetype = Condition), size = 1.2, position=pd) +
  geom_errorbar(aes(ymin= RespCorr - ci, ymax= RespCorr + ci), width= 0.2, size = 1, position=pd)+
  scale_color_manual(values= c("#0066CC", "#CC0033","#33FF00", "#000000")) +
  labs(x="Block", y="Proportion of Correct Responses") +
  scale_y_continuous(limits = c(0.45,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  facet_grid(~TrialType2, scales="free", space= "free_x") +
  theme_alan() +
  ggtitle("Comparing Arbitrary  6")

#GGSAVE
ggsave("C:/Users/Alank/Documents/GitHub/Stekic-et-al/Figures/Arbitrary Condtions (7,8,9).png", plot = last_plot(), device = NULL, path = NULL,
  width = 8, height = 4, units = c("in", "cm", "mm"),
  dpi = 600)


#COUNTS
SubCountsArb <- data.frame(count(ArbSub, Condition))
SubCountsArb$n <- SubCountsArb$n/240
SubCountsArb %>% 
            knitr::kable(caption = "Number of Participants in Subconditions of Experiment 6", ) %>%
              kable_styling(full_width= F)
```

The case to collapse these ones is even more clear - there is well and truly no difference between them (and we don't expect one)

Now we can move on to the main analysis of Part III, where we're looking at all the Item-Label Conditions

```{r Visualising Part III}

# Subsetting out only the conditions for the replication
Part3Data <- subset(CleanData.Final, Condition == 10|Condition == 4|Condition == 5|Condition == "6A"|Condition == "6B"
                    |Condition == 7|Condition == 8|Condition == 9)

# Name the levels of Condition, rather than having them be numbers
# This also collapses subconditions 3A and 3B (justified above)
Part3Data$Condition <- factor(Part3Data$Condition,
                            levels= c(10, 4, 5, "6A", "6B", 7, 8, 9),
                            labels = c("No Label", "Iconic (Item)", "Counter-Iconic (Item)", "Conventional (Item)", "Conventional (Item)",
                                       rep("Arbitrary",3)))

#Aggregate our data for plotting
Part3Agg <- summarySE(Part3Data, measurevar = "RespCorr", groupvars = c("Condition", "TrialType2", "Block"))
Part3Agg$Block <- factor(Part3Agg$Block)

#Plot our data
ggplot(data=Part3Agg, aes(x=Block, y=RespCorr, group= Condition)) +
  geom_line(aes(linetype = Condition, color = Condition), size = 1.2, position=pd) +
  geom_errorbar(aes(ymin= RespCorr - ci, ymax= RespCorr + ci, color= Condition), width= 0.2, size = 1, position=pd)+
  scale_color_manual(values= c("#000000", "#c144c1","#8b2e8b", "#652165", "gray")) +
  scale_linetype_manual(values = c(rep("dashed",5))) +
  labs(x="Block", y="Proportion of Correct Responses") +
  scale_y_continuous(limits = c(0.40,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  facet_grid(~TrialType2, scales="free", space= "free_x") +
  theme_alan() +
  ggtitle("Individual Label Comparison")

#GGSAVE
ggsave("C:/Users/Alank/Documents/GitHub/Stekic-et-al/Figures/Individual Labels.png", plot = last_plot(), device = NULL, path = NULL,
  width = 8, height = 4, units = c("in", "cm", "mm"),
  dpi = 600)

```

```{r Part 3 Statistics - Main Effects}

afex.Part3 <- mixed(RespCorr ~ Condition * TrialType2 + (1|ParticipantID),
                         data=Part3Data,
                         family=binomial,
                         control=glmerControl(optimizer="bobyqa"),
                         method = 'LRT')


#Pretty up results and kick out as kable
afex.Part3$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Mixed Model Results - Part 2", ) %>%
              kable_styling(full_width= F)

##Fixed Effects 
summary(afex.Part3)$coefficients  %>%
  data.frame() %>%
    tibble::rownames_to_column(var= "FixedEffect") %>% 
    mutate(FixedEffect = recode_factor(FixedEffect, 
                                       `Condition1` = "Iconic",
                                        `Condition2` = "Counter-Iconic",
                                       `Condition3` = "Conventional",
                                       `Condition4` = "Arbitrary",
                                       `TrialType21` = "Testing - Old",
                                       `TrialType22` = "Testing - New",
                                       `Condition1:TrialType21` = "Iconic x Testing - Old",
                                       `Condition2:TrialType21` = "Counter-Iconic x Testing - Old",
                                       `Condition3:TrialType21` = "Conventional x Testing - Old",
                                       `Condition4:TrialType21` = "Arbitrary x Testing - Old",
                                       `Condition1:TrialType22` = "Iconic x Testing - New",
                                       `Condition2:TrialType22` = "Counter-Iconic x Testing - New",
                                       `Condition3:TrialType22` = "Conventional x Testing - New", 
                                       `Condition4:TrialType22` = "Arbitrary x Testing - New"
                                       )) %>%
      `colnames<-`(c("Fixed Effect", "Estimate", "SE", "z", "p")) %>%
        mutate_if(is.numeric, round, digits = 3) %>% 
          mutate_at("p", funs(ifelse(.<0.001, "<0.001", .))) %>%
            knitr::kable(caption = "Word Type Comparison- Fixed Effects Table", ) %>%
              kable_styling(full_width= F) %>%
                pack_rows("Intercept", 1, 1) %>%
                  pack_rows("Condition", 2, 5) %>%
                    pack_rows("Trial Type", 6, 7)%>%
                      pack_rows("Condition x Trial Type Interactions", 8, 15)


```
```{r Part III - Post Hoc Interactions}


Part3.Interaction.vPairs <- data.frame(pairs(emmeans(afex.Part3, 
                    ~Condition * TrialType2,
                    adjust = "none")))

#TRAINING 
Part3.Interaction.vPairs.Training.1 <-   rbind(rep(NA,6),Part3.Interaction.vPairs[1:4,])
Part3.Interaction.vPairs.Training.2 <-   rbind(Part3.Interaction.vPairs[1,], rep(NA, 6), Part3.Interaction.vPairs[15:17,])
Part3.Interaction.vPairs.Training.3 <-   rbind(Part3.Interaction.vPairs[2,], Part3.Interaction.vPairs[15,], rep(NA, 6),
                                               Part3.Interaction.vPairs[28:29,])
Part3.Interaction.vPairs.Training.4 <-   rbind(Part3.Interaction.vPairs[3,], Part3.Interaction.vPairs[16,], Part3.Interaction.vPairs[28,], 
                                               rep(NA, 6), Part3.Interaction.vPairs[40,])
Part3.Interaction.vPairs.Training.5 <-    rbind(Part3.Interaction.vPairs[4,], Part3.Interaction.vPairs[17,], Part3.Interaction.vPairs[29,], 
                                               Part3.Interaction.vPairs[40,], rep(NA, 6))

#TESTING OLD
Part3.Interaction.vPairs.TestingOld.1 <-   rbind(rep(NA,6),Part3.Interaction.vPairs[61:64,])
Part3.Interaction.vPairs.TestingOld.2 <-   rbind(Part3.Interaction.vPairs[61,], rep(NA, 6), Part3.Interaction.vPairs[70:72,])
Part3.Interaction.vPairs.TestingOld.3 <-   rbind(Part3.Interaction.vPairs[62,], Part3.Interaction.vPairs[70,], rep(NA, 6),
                                               Part3.Interaction.vPairs[78:79,])
Part3.Interaction.vPairs.TestingOld.4 <-   rbind(Part3.Interaction.vPairs[63,], Part3.Interaction.vPairs[71,], Part3.Interaction.vPairs[78,], 
                                               rep(NA, 6), Part3.Interaction.vPairs[85,])
Part3.Interaction.vPairs.TestingOld.5 <-    rbind(Part3.Interaction.vPairs[64,], Part3.Interaction.vPairs[72,], Part3.Interaction.vPairs[79,], 
                                               Part3.Interaction.vPairs[85,], rep(NA, 6))

#TESTING NEW
Part3.Interaction.vPairs.TestingNew.1 <-   rbind(rep(NA,6),Part3.Interaction.vPairs[96:99,])
Part3.Interaction.vPairs.TestingNew.2 <-   rbind(Part3.Interaction.vPairs[96,], rep(NA, 6), Part3.Interaction.vPairs[100:102,])
Part3.Interaction.vPairs.TestingNew.3 <-   rbind(Part3.Interaction.vPairs[97,], Part3.Interaction.vPairs[100,], rep(NA, 6),
                                               Part3.Interaction.vPairs[103:104,])
Part3.Interaction.vPairs.TestingNew.4 <-   rbind(Part3.Interaction.vPairs[98,], Part3.Interaction.vPairs[101,], Part3.Interaction.vPairs[103,], 
                                               rep(NA, 6), Part3.Interaction.vPairs[105,])
Part3.Interaction.vPairs.TestingNew.5 <-    rbind(Part3.Interaction.vPairs[99,], Part3.Interaction.vPairs[102,], Part3.Interaction.vPairs[104,], 
                                               Part3.Interaction.vPairs[105,], rep(NA, 6))

#COMPARISONS OF CONDITIONS WITHIN TRAINING TRIALS
Part3.Interaction.Training <- cbind.data.frame(
  levels(Part3Data$Condition), #Conditions
  as.vector(tapply(subset(Part3Data, TrialType2 == "Training")$RespCorr, subset(Part3Data, TrialType2 == "Training")$Condition, mean)), #Means Within Training 
  Part3.Interaction.vPairs.Training.1$p.value,
  Part3.Interaction.vPairs.Training.2$p.value,
  Part3.Interaction.vPairs.Training.3$p.value,
  Part3.Interaction.vPairs.Training.4$p.value,
  Part3.Interaction.vPairs.Training.5$p.value
)

Part3.Interaction.Training %>% 
    `colnames<-`(c("Condition", "Proportion Correct", "v NL", "v I", "v CI", "v Con", "v Arb")) %>%
    mutate_if(is.numeric, round, 3) %>%
    mutate_at(.vars = vars("v NL", "v I", "v CI", "v Con", "v Arb"), funs(ifelse(.<0.001, "<0.001", .))) %>%
          knitr::kable(caption = 'Part III - Post Hoc Comparisons of Condition within Training Trials', "html") %>%
            kable_styling(full_width = F) %>%
              add_header_above(c(" " = 2, "Post Hoc Comparisons (p values)" = 5),
                   font_size = 12) 

#COMPARISONS OF CONDITIONS WITHIN TESTING TRIALS
Part3.Interaction.Testing <- cbind.data.frame(
  levels(Part3Data$Condition), #Conditions
  as.vector(tapply(subset(Part3Data, TrialType == "Testing")$RespCorr, subset(Part3Data, TrialType == "Testing")$Condition, mean)), #Means Within Testing 
  Part3.Interaction.vPairs.TestingOld.1$p.value,
  Part3.Interaction.vPairs.TestingOld.2$p.value,
  Part3.Interaction.vPairs.TestingOld.3$p.value,
  Part3.Interaction.vPairs.TestingOld.4$p.value,
  Part3.Interaction.vPairs.TestingOld.5$p.value,
  Part3.Interaction.vPairs.TestingNew.1$p.value,
  Part3.Interaction.vPairs.TestingNew.2$p.value,
  Part3.Interaction.vPairs.TestingNew.3$p.value,
  Part3.Interaction.vPairs.TestingNew.4$p.value,
  Part3.Interaction.vPairs.TestingNew.5$p.value
)

Part3.Interaction.Testing %>% 
    `colnames<-`(c("Condition", "Proportion Correct", "v NL (O)", "v I (O)", "v CI (O)", "v Con (O)", "v Arb (O)",
                   "v NL (N)", "v I (N)", "v CI (N)", "v Con (N)", "v Arb (N)")) %>%
    mutate_if(is.numeric, round, 3) %>%
    mutate_at(.vars = vars("v NL (O)", "v I (O)", "v CI (O)", "v Con (O)", "v Arb (O)",
                   "v NL (N)", "v I (N)", "v CI (N)", "v Con (N)", "v Arb (N)"), funs(ifelse(.<0.001, "<0.001", .))) %>%
          knitr::kable(caption = 'Part III - Post Hoc Comparisons of Condition within Testing Trials', "html") %>%
            kable_styling(full_width = F) %>%
              add_header_above(c(" " = 2, "Testing - Old Trials" = 5, "Testing - New Trials" = 5),
                   font_size = 12) %>%
                add_header_above(c(" " = 2,  "Post Hoc Comparisons (p values)" = 10), 
                                 font_size = 14)

```

Same story as for Part 2 (above). The Interaction effect is explained by the fact that the Iconic, Counter-Iconic, and Conventional label learners perform significantly better (but not differently from each other) during Training trials than participants in the No Label or Arbitrary Label Conditions (who do not perform different from each other).

In testing trials, there are no post-hoc differences - (there is a single point but it isn't very interesting) between the conditions


```{r RT Analysis for PART III}
afex.Part3.RT <- mixed(RT ~ Condition * TrialType2 + (1|ParticipantID),
                         data=Part3Data,
                         method = 'LRT')


#Pretty up results and kick out as kable
afex.Part3.RT$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Mixed Model Results - RTs - Part II", ) %>%
              kable_styling(full_width= F)

##Fixed Effects 
summary(afex.Part3.RT)$coefficients  %>%
  data.frame() %>%
    tibble::rownames_to_column(var= "FixedEffect") %>% 
    mutate(FixedEffect = recode_factor(FixedEffect, 
                                       `Condition1` = "Iconic",
                                        `Condition2` = "Counter-Iconic",
                                       `Condition3` = "Conventional",
                                       `Condition4` = "Arbitrary",
                                       `TrialType21` = "Testing - Old",
                                       `TrialType22` = "Testing - New",
                                       `Condition1:TrialType21` = "Iconic x Testing - Old",
                                       `Condition2:TrialType21` = "Counter-Iconic x Testing - Old",
                                       `Condition3:TrialType21` = "Conventional x Testing - Old",
                                       `Condition4:TrialType21` = "Arbitrary x Testing - Old",
                                       `Condition1:TrialType22` = "Iconic x Testing - New",
                                       `Condition2:TrialType22` = "Counter-Iconic x Testing - New",
                                       `Condition3:TrialType22` = "Conventional x Testing - New", 
                                       `Condition4:TrialType22` = "Arbitrary x Testing - New"
                                       )) %>%
      `colnames<-`(c("Fixed Effect", "Estimate", "SE", "df", "t", "p")) %>%
        mutate_if(is.numeric, round, digits = 3) %>% 
          mutate_at("p", funs(ifelse(.<0.001, "<0.001", .))) %>%
            knitr::kable(caption = "Word Type Comparison- Fixed Effects Table - RTs - Part 2", ) %>%
              kable_styling(full_width= F) %>%
                pack_rows("Intercept", 1, 1) %>%
                  pack_rows("Condition", 2, 5) %>%
                    pack_rows("Trial Type", 6, 7)%>%
                      pack_rows("Condition x Trial Type Interactions", 8, 15)


Part3AggRT <- summarySE(Part3Data, measurevar = "RT", groupvars = c("Condition", "TrialType2", "Block"))

Part3AggRT$Block <- factor(Part3AggRT$Block)

#Plot our data
ggplot(data=Part3AggRT, aes(x=Block, y=RT, group= Condition)) +
  geom_line(aes(linetype = Condition, color = Condition), size = 1.2, position=pd) +
  geom_errorbar(aes(ymin= RT - ci, ymax= RT + ci, color= Condition), width= 0.2, size = 1, position=pd)+
  scale_color_manual(values= c("#000000", "#c144c1","#8b2e8b", "#652165", "gray")) +
  scale_linetype_manual(values = c(rep("dashed",5))) +
  labs(x="Block", y="Response Time") +
  #scale_y_continuous(limits = c(0.40,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  facet_grid(~TrialType2, scales="free", space= "free_x") +
  theme_alan() +
  ggtitle("Response Time Comparison - Part 3")

#GGSAVE
ggsave("C:/Users/Alank/Documents/GitHub/Stekic-et-al/Figures/Response Time - Part 3.png", plot = last_plot(), device = NULL, path = NULL,
  width = 8, height = 4, units = c("in", "cm", "mm"),
  dpi = 600)


```

```{r Part 2 - Block Analysis for Training Trials}

afex.Part3.Block <- mixed(RespCorr ~ Condition * Block + (1|ParticipantID),
                         data=subset(Part3Data, TrialType2 == "Training") ,
                        method = 'LRT')

afex.Part3.Block$anova_table


```


# PART IV - THE WHOLE SHEBANG

```{r I'ma converge this whole house}

#CHANGED THIS SO WE DON"T HAVE NO LABEL DATA READ IN TWICE
Part4Data <- rbind(subset(Part2Data, Condition != "No Label"), Part3Data)

afex.Part4 <- mixed(RespCorr ~ Condition * TrialType2 + (1|ParticipantID),
                         data=Part4Data,
                         family=binomial,
                         control=glmerControl(optimizer="bobyqa"),
                         method = 'LRT')

afex.Part4$anova_table


#Pretty up results and kick out as kable
afex.Part4$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Mixed Model Results - Part 2", ) %>%
              kable_styling(full_width= F)

##Fixed Effects 
summary(afex.Part4)$coefficients  %>%
  data.frame() %>%
    tibble::rownames_to_column(var= "FixedEffect") %>% 
    mutate(FixedEffect = recode_factor(FixedEffect, 
                                       `Condition1` = "Iconic (Category)",
                                        `Condition2` = "Counter-Iconic (Category)",
                                       `Condition3` = "Conventional (Category)",
                                       `Condition4` = "Iconic (Item)",
                                       `Condition5` = "Counter-Iconic (Item)",
                                       `Condition6` = "Conventional (Item)",
                                       `Condition7` = "Arbitrary",
                                       `TrialType21` = "Testing - Old",
                                       `TrialType22` = "Testing - New",
                                       `Condition1:TrialType21` = "Iconic (Category) x Testing - Old",
                                       `Condition2:TrialType21` = "Counter-Iconic (Category) x Testing - Old",
                                       `Condition3:TrialType21` = "Conventional (Category) x Testing - Old",
                                       `Condition4:TrialType21` = "Iconic (Item) x Testing - Old",
                                       `Condition5:TrialType21` = "Counter-Iconic (Item) x Testing - Old",
                                       `Condition6:TrialType21` = "Conventional (Item) x Testing - Old",
                                       `Condition7:TrialType21` = "Arbitrary x Testing - Old",
                                       `Condition1:TrialType22` = "Iconic (Category) x Testing - New",
                                       `Condition2:TrialType22` = "Counter-Iconic (Category) x Testing - New",
                                       `Condition3:TrialType22` = "Conventional (Category) x Testing - New",
                                       `Condition4:TrialType22` = "Iconic (Item) x Testing - New",
                                       `Condition5:TrialType22` = "Counter-Iconic (Item) x Testing - New",
                                       `Condition6:TrialType22` = "Conventional (Item) x Testing - New",
                                       `Condition7:TrialType22` = "Arbitrary x Testing - New"
                                       )) %>%
      `colnames<-`(c("Fixed Effect", "Estimate", "SE", "z", "p")) %>%
        mutate_if(is.numeric, round, digits = 3) %>% 
          mutate_at("p", funs(ifelse(.<0.001, "<0.001", .))) %>%
            knitr::kable(caption = "Word Type Comparison- Fixed Effects Table", ) %>%
              kable_styling(full_width= F) %>%
                pack_rows("Intercept", 1, 1) %>%
                  pack_rows("Condition", 2, 8) %>%
                    pack_rows("Trial Type", 9, 10)%>%
                      pack_rows("Condition x Trial Type Interactions", 11, 24)

Part4Agg <- summarySE(Part4Data, measurevar = "RespCorr", groupvars = c("Condition", "TrialType2", "Block"))
Part4Agg$Block <- factor(Part4Agg$Block)

#Plot our data
ggplot(data=Part4Agg, aes(x=Block, y=RespCorr, group= Condition)) +
  geom_line(aes(linetype = Condition, color = Condition), size = 1.2, position=pd) +
  geom_errorbar(aes(ymin= RespCorr - ci, ymax= RespCorr + ci, color= Condition), width= 0.2, size = 1, position=pd)+
  scale_color_manual(values= c("#000000", "#007e70","#00b19d", "#00e4ca", "#c144c1","#8b2e8b", "#652165", "gray")) +
  scale_linetype_manual(values = c("dashed", rep("solid", 3), rep("dashed",4))) +
  labs(x="Block", y="Proportion of Correct Responses") +
  scale_y_continuous(limits = c(0.40,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  facet_grid(~TrialType2, scales="free", space= "free_x") +
  theme_alan() +
  ggtitle("Omnibus Comparison")

#GGSAVE
ggsave("C:/Users/Alank/Documents/GitHub/Stekic-et-al/Figures/Omnibus Plot (All Conditions).png", plot = last_plot(), device = NULL, path = NULL,
  width = 8, height = 4, units = c("in", "cm", "mm"),
  dpi = 600)



```

```{r RT Analysis for PART IV}
afex.Part4.RT <- mixed(RT ~ Condition * TrialType2 + (1|ParticipantID),
                         data=Part4Data,
                         method = 'LRT')


#Pretty up results and kick out as kable
afex.Part4.RT$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Mixed Model Results - RTs - Part II", ) %>%
              kable_styling(full_width= F)

##Fixed Effects 
summary(afex.Part4.RT)$coefficients  %>%
  data.frame() %>%
    tibble::rownames_to_column(var= "FixedEffect") %>% 
    mutate(FixedEffect = recode_factor(FixedEffect, 
                                       `Condition1` = "Iconic (Category)",
                                        `Condition2` = "Counter-Iconic (Category)",
                                       `Condition3` = "Conventional (Category)",
                                       `Condition4` = "Iconic (Item)",
                                       `Condition5` = "Counter-Iconic (Item)",
                                       `Condition6` = "Conventional (Item)",
                                       `Condition7` = "Arbitrary",
                                       `TrialType21` = "Testing - Old",
                                       `TrialType22` = "Testing - New",
                                       `Condition1:TrialType21` = "Iconic (Category) x Testing - Old",
                                       `Condition2:TrialType21` = "Counter-Iconic (Category) x Testing - Old",
                                       `Condition3:TrialType21` = "Conventional (Category) x Testing - Old",
                                       `Condition4:TrialType21` = "Iconic (Item) x Testing - Old",
                                       `Condition5:TrialType21` = "Counter-Iconic (Item) x Testing - Old",
                                       `Condition6:TrialType21` = "Conventional (Item) x Testing - Old",
                                       `Condition7:TrialType21` = "Arbitrary x Testing - Old",
                                       `Condition1:TrialType22` = "Iconic (Category) x Testing - New",
                                       `Condition2:TrialType22` = "Counter-Iconic (Category) x Testing - New",
                                       `Condition3:TrialType22` = "Conventional (Category) x Testing - New",
                                       `Condition4:TrialType22` = "Iconic (Item) x Testing - New",
                                       `Condition5:TrialType22` = "Counter-Iconic (Item) x Testing - New",
                                       `Condition6:TrialType22` = "Conventional (Item) x Testing - New",
                                       `Condition7:TrialType22` = "Arbitrary x Testing - New"
                                       )) %>%
      `colnames<-`(c("Fixed Effect", "Estimate", "SE", "df", "t", "p")) %>%
        mutate_if(is.numeric, round, digits = 3) %>% 
          mutate_at("p", funs(ifelse(.<0.001, "<0.001", .))) %>%
            knitr::kable(caption = "Word Type Comparison- Fixed Effects Table - RTs - Part 2", ) %>%
              kable_styling(full_width= F) %>%
                pack_rows("Intercept", 1, 1) %>%
                  pack_rows("Condition", 2, 8) %>%
                    pack_rows("Trial Type", 9, 10)%>%
                      pack_rows("Condition x Trial Type Interactions", 11, 24)


Part4AggRT <- summarySE(Part4Data, measurevar = "RT", groupvars = c("Condition", "TrialType2", "Block"))

Part4AggRT$Block <- factor(Part4AggRT$Block)

#Plot our data
ggplot(data=Part4AggRT, aes(x=Block, y=RT, group= Condition)) +
  geom_line(aes(linetype = Condition, color = Condition), size = 1.2, position=pd) +
  geom_errorbar(aes(ymin= RT - ci, ymax= RT + ci, color= Condition), width= 0.2, size = 1, position=pd)+
  scale_color_manual(values= c("#000000", "#007e70","#00b19d", "#00e4ca", "#c144c1","#8b2e8b", "#652165", "gray")) +
  scale_linetype_manual(values = c("dashed", rep("solid", 3), rep("dashed",4))) +
  labs(x="Block", y="Response Time") +
  #scale_y_continuous(limits = c(0.40,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  facet_grid(~TrialType2, scales="free", space= "free_x") +
  theme_alan() +
  ggtitle("Response Time Comparison - Part 4")

#GGSAVE
ggsave("C:/Users/Alank/Documents/GitHub/Stekic-et-al/Figures/Response Time - Part 4.png", plot = last_plot(), device = NULL, path = NULL,
  width = 8, height = 4, units = c("in", "cm", "mm"),
  dpi = 600)


```

We're going to try out a model of Iconicity x Systematicity with the data that we do have available, looking only at Training trials, and see if we can get something to converge


```{r A model of iconicity x systematicity}


conditions <- levels(Part4Data$Condition)

Part4Data$LabelType <- mapvalues(Part4Data$Condition,
                                 from = conditions,
                                 to = c("None", rep("Category", 3), rep("Item", 4)))

Part4Data$Systematicity <- mapvalues(Part4Data$Condition,
                                 from = conditions,
                                 to = c("None", rep("Non-Systematic", 3), rep("Systematic", 3), "Non-Systematic"))


Part4Data$Iconicity <- mapvalues(Part4Data$Condition,
                                 from = conditions,
                                 to = c("None", "Iconic", "Counter-Iconic", "Conventional" , "Iconic", "Counter-Iconic", 
                                        "Conventional", "Conventional"))

#Kick out a Crossings Table for the Curly-Brained in the Audience
Crossings <- cbind.data.frame(conditions, 
                              c("None", rep("Category", 3), rep("Item", 4)),
                              c("None", rep("Non-Systematic", 3), rep("Systematic", 3), "Non-Systematic"),
                              c("None", "Iconic", "Counter-Iconic", "Conventional" , "Iconic", "Counter-Iconic", 
                                        "Conventional", "Conventional")
                              )
Crossings %>% 
          `colnames<-`(c("Condition", "Label Type", "Systematicity", "Iconicity")) %>%
            knitr::kable(caption = "Crossings of Factors", ) %>%
              kable_styling(full_width= F)

                           

afex.Part4.NewModel <- mixed(RT ~ LabelType + Systematicity + Iconicity + (1|ParticipantID),
                         data=subset(Part4Data, TrialType2 == "Training"),
                         method = 'LRT')


#Pretty up results and kick out as kable
afex.Part4.NewModel$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Part 4 - Analysis of Factors", ) %>%
              kable_styling(full_width= F)



Part4AggNew <- summarySE(Part4Data, measurevar = "RespCorr", groupvars = c("LabelType", "Systematicity", "Iconicity", "Block", "Condition"))
Part4AggNew$Block <- factor(Part4AggNew$Block)

#Plot our data
ggplot(data=Part4AggNew, aes(x=Block, y=RespCorr, group= Condition)) +
  geom_line(aes(linetype = Systematicity, color = Iconicity), size = 1.2, position=pd) +
  geom_errorbar(aes(ymin= RespCorr - ci, ymax= RespCorr + ci, color= Iconicity), width= 0.2, size = 1, position=pd)+
  scale_color_manual(values= c("#000000", "#007e70","#00b19d", "#00e4ca", "#c144c1","#8b2e8b", "#652165", "gray")) +
  scale_linetype_manual(values = c("dashed", rep("solid", 3), rep("dashed",4))) +
  labs(x="Block", y="Proportion of Correct Responses") +
  scale_y_continuous(limits = c(0.40,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  facet_grid(~LabelType, scales="free", space= "free_x") +
  theme_alan() +
  ggtitle("Omnibus Comparison")

```


# Alan's Extra Bits and Bobs and Impressive things because he's a super cool guy

## Data Exclusion Table

Lets look at the number of participants who were excluded along the way

```{r Data Exclusion Table}

exclusions <- c("All Data Collected",
                "NA Response Value", 
                "Negative RT Values", 
                "High RT Values",
                "Spammers",
                "Cowards/Bravos",
                "RT Extremes (by Trial)",
                "Final"
                )

trialnums <- c(nrow(CleanData), #All Data
               nrow(CleanData.RespCorr), #NA Participant
               nrow(CleanData.RTNegCorr), #NEgative RT Values
               nrow(CleanData2), #High RT Values
               nrow(CleanData5), #Spammers
               nrow(CleanData6), #Cowards/Bravos
               nrow(CleanData.RTTrimmed), #Long Trials
               nrow(CleanData.Final)
               )

participantnums <- c(nrow(CleanData)/240, #All Data
               nrow(CleanData.RespCorr)/240, #NA Participant
               nrow(CleanData.RTNegCorr)/240, #NEgative RT Values
               nrow(CleanData2)/240, #High RT Values
               nrow(CleanData5)/240, #Spammers
               nrow(CleanData6)/240, #Cowards/Bravos
               nrow(CleanData6)/240, #Long Trials
               nrow(CleanData6)/240 #we don't lose participants for the last cut so we use the value from above
               )

trialscut <- c(NA,
               nrow(CleanData)- nrow(CleanData.RespCorr), #NA Participant
               nrow(CleanData.RespCorr) - nrow(CleanData.RTNegCorr), #NEgative RT Values
               nrow(CleanData.RTNegCorr) - nrow(CleanData2), #High RT Values
               nrow(CleanData2) - nrow(CleanData5), #Spammers
               nrow(CleanData5) - nrow(CleanData6), #Cowards/Bravos
               nrow(CleanData6) - nrow(CleanData.RTTrimmed), #Long Trials
               NA
               )

participantscut <- c(NA, #All Data
               nrow(CleanData)/240 - nrow(CleanData.RespCorr)/240, #NA Participant
               nrow(CleanData.RespCorr)/240 - nrow(CleanData.RTNegCorr)/240, #NEgative RT Values
               nrow(CleanData.RTNegCorr)/240 - nrow(CleanData2)/240, #High RT Values
               nrow(CleanData2)/240 - nrow(CleanData5)/240, #Spammers
               nrow(CleanData5)/240 - nrow(CleanData6)/240, #Cowards/Bravos
               NA, #Long Trials
               NA #we don't lose participants for the last cut so we use the value from above
               )


ExclusionsTable <- cbind.data.frame(exclusions, participantnums, trialnums, participantscut, trialscut)

ExclusionsTable %>%  
      `colnames<-`(c("Exclusion Criteria", "Participants", "Trials", "Participants Excluded", "Trials Excluded"))%>% 
      mutate_if(is.numeric, round, digits = 0) %>% 
            knitr::kable(caption = "Exclusion Table - Participants and Trials Cut by Exclusion Criteria" ) %>%
              kable_styling(full_width= F)


```


So there is the exclusion table.

What about the balancing of COnditions within the retracted samples

```{r Condition Counts }

exclusions2 <- c("All Data Collected",
                "NA Response Value", 
                "Negative RT Values", 
                "High RT Values",
                "Spammers",
                "Cowards/Bravos",
                "Final"
                )

participantnums2 <- c(nrow(CleanData)/240, #All Data
               nrow(CleanData.RespCorr)/240, #NA Participant
               nrow(CleanData.RTNegCorr)/240, #NEgative RT Values
               nrow(CleanData2)/240, #High RT Values
               nrow(CleanData5)/240, #Spammers
               nrow(CleanData6)/240, #Cowards/Bravos
               nrow(CleanData6)/240 #we don't lose participants for the last cut so we use the value from above
               )




AllDataCounts <- CleanData %>%
   mutate(Condition2 = 
      factor(Condition, levels = c (1,2,"3A", "3B", 4, 5, "6A", "6B", 7, 8, 9, 10),
                        labels = c("Iconic (Category)", "Counter-Iconic (Category)", "Conventional (Category)", "Conventional (Category)",
                                  "Iconic (Item)", "Counter-Iconic (Item)", "Conventional (Item)", "Conventional (Item)",
                                  "Arbitrary", "Arbitrary", "Arbitrary",
                                  "No Label"
                                  ))) %>%
        count(Condition2, sort = FALSE) 
  

NADataCounts <- CleanData.RespCorr %>%
   mutate(Condition2 = 
      factor(Condition, levels = c (1,2,"3A", "3B", 4, 5, "6A", "6B", 7, 8, 9, 10),
                        labels = c("Iconic (Category)", "Counter-Iconic (Category)", "Conventional (Category)", "Conventional (Category)",
                                  "Iconic (Item)", "Counter-Iconic (Item)", "Conventional (Item)", "Conventional (Item)",
                                  "Arbitrary", "Arbitrary", "Arbitrary",
                                  "No Label"
                                  ))) %>%
        count(Condition2, sort = FALSE) 


NegRTDataCounts <- CleanData.RTNegCorr %>%
   mutate(Condition2 = 
      factor(Condition, levels = c (1,2,"3A", "3B", 4, 5, "6A", "6B", 7, 8, 9, 10),
                        labels = c("Iconic (Category)", "Counter-Iconic (Category)", "Conventional (Category)", "Conventional (Category)",
                                  "Iconic (Item)", "Counter-Iconic (Item)", "Conventional (Item)", "Conventional (Item)",
                                  "Arbitrary", "Arbitrary", "Arbitrary",
                                  "No Label"
                                  ))) %>%
        count(Condition2, sort = FALSE) 


HighRTDataCounts <- CleanData2 %>%
   mutate(Condition2 = 
      factor(Condition, levels = c (1,2,"3A", "3B", 4, 5, "6A", "6B", 7, 8, 9, 10),
                        labels = c("Iconic (Category)", "Counter-Iconic (Category)", "Conventional (Category)", "Conventional (Category)",
                                  "Iconic (Item)", "Counter-Iconic (Item)", "Conventional (Item)", "Conventional (Item)",
                                  "Arbitrary", "Arbitrary", "Arbitrary",
                                  "No Label"
                                  ))) %>%
        count(Condition2, sort = FALSE) 

SpammersDataCounts <- CleanData5 %>%
   mutate(Condition2 = 
      factor(Condition, levels = c (1,2,"3A", "3B", 4, 5, "6A", "6B", 7, 8, 9, 10),
                        labels = c("Iconic (Category)", "Counter-Iconic (Category)", "Conventional (Category)", "Conventional (Category)",
                                  "Iconic (Item)", "Counter-Iconic (Item)", "Conventional (Item)", "Conventional (Item)",
                                  "Arbitrary", "Arbitrary", "Arbitrary",
                                  "No Label"
                                  ))) %>%
        count(Condition2, sort = FALSE) 


CowardsDataCounts <- CleanData6 %>%
   mutate(Condition2 = 
      factor(Condition, levels = c (1,2,"3A", "3B", 4, 5, "6A", "6B", 7, 8, 9, 10),
                        labels = c("Iconic (Category)", "Counter-Iconic (Category)", "Conventional (Category)", "Conventional (Category)",
                                  "Iconic (Item)", "Counter-Iconic (Item)", "Conventional (Item)", "Conventional (Item)",
                                  "Arbitrary", "Arbitrary", "Arbitrary",
                                  "No Label"
                                  ))) %>%
        count(Condition2, sort = FALSE) 


ConditionCounts <- rbind.data.frame(AllDataCounts[1:8,]$n/240, NADataCounts$n/240, NegRTDataCounts$n/240,
                        HighRTDataCounts$n/240, SpammersDataCounts$n/240, CowardsDataCounts$n/240, CowardsDataCounts$n/240) 

ConditionCountsTable <- cbind.data.frame(exclusions2, participantnums2, ConditionCounts)

ConditionCountsTable %>%  
      `colnames<-`(c("Exclusion Criteria", "Total Participants", 
                     "Iconic (Category)", "Counter-Iconic (Category)", "Conventional (Category)", 
                                  "Iconic (Item)", "Counter-Iconic (Item)", "Conventional (Item)", 
                                  "Arbitrary",
                                  "No Label"
                     ))%>% 
      mutate_if(is.numeric, round, digits = 0) %>% 
            knitr::kable(caption = "Exclusion Table - Participants per Condition at each Exclusion Level" ) %>%
              kable_styling(full_width= F)


```



Quick check of subconditions by condition

```{r SUbconditions x Condition}

subcountstable <- data.frame(count(CleanData.Final, Condition, Subcondition))
subcountstable$n <- subcountstable$n/240

subcountstable %>% 
      `colnames<-`(c("Condition", "Approach Shape", "n"))%>% 
      mutate_if(is.numeric, round, digits = 0) %>% 
            knitr::kable(caption = "Subcondition Balancing by Condition" ) %>%
              kable_styling(full_width= F)



```


Looks good. Groovy. Moving on

## Early Trial Analysis

Lets take a look at the very earliest Training trials and see if we can recover any benefits for iconicity (this is where we'd expect to find them)

```{r Early Trial Analysis}

#FIRST 4 TRIALS
OmnibusEarly4 <- subset(Exp4Data, TrialNum < 5)

OmnibusEarly4Agg <- summarySE(OmnibusEarly4, measurevar = "RespCorr", groupvars = c("Condition"))

#Plot our data
ggplot(data = OmnibusEarly4Agg, aes(x= Condition, y= RespCorr)) +
  geom_bar(stat = "summary", fun.y = "mean", aes(fill = Condition), position = position_dodge(width=0.9)) +
  geom_errorbar(aes(ymin= RespCorr - ci, ymax= RespCorr + ci, color= Condition), width= 0.2, size = 1, position=pd)+
  #scale_color_manual(values= c("#000000", "#007e70","#00b19d", "#00e4ca", "#c144c1","#8b2e8b", "#652165", "gray")) +
  #scale_linetype_manual(values = c("dashed", rep("solid", 3), rep("dashed",4))) +
  labs(x="Condition", y="Proportion of Correct Responses") +
  #scale_y_continuous(limits = c(0.40,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  theme_alan() +
  ggtitle("Early Trial Analysis - First 4 Trials by Condition")

#GGSAVE
ggsave("C:/Users/Alank/Documents/GitHub/Stekic-et-al/Figures/Early Trial Analysis by Condition.png", plot = last_plot(), device = NULL, path = NULL,
  width = 12, height = 6, units = c("in", "cm", "mm"),
  dpi = 600)


#FIRST 4 TRIALS BY ICONICITY (COLLAPSED)
OmnibusEarly4Agg2 <- summarySE(OmnibusEarly4, measurevar = "RespCorr", groupvars = c("Iconicity"))

#Plot our data
ggplot(data = OmnibusEarly4Agg2, aes(x= Iconicity, y= RespCorr)) +
  geom_bar(stat = "summary", fun.y = "mean", aes(fill = Iconicity), position = position_dodge(width=0.9)) +
  geom_errorbar(aes(ymin= RespCorr - ci, ymax= RespCorr + ci, color= Iconicity), width= 0.2, size = 1, position=pd)+
  #scale_color_manual(values= c("#000000", "#007e70","#00b19d", "#00e4ca", "#c144c1","#8b2e8b", "#652165", "gray")) +
  #scale_linetype_manual(values = c("dashed", rep("solid", 3), rep("dashed",4))) +
  labs(x="Iconicity", y="Proportion of Correct Responses") +
  #scale_y_continuous(limits = c(0.40,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  theme_alan() +
  ggtitle("Early Trial Analysis - First 4 Trials by Iconicity")

#GGSAVE
ggsave("C:/Users/Alank/Documents/GitHub/Stekic-et-al/Figures/Early Trial Analysis by Iconicity (4 Trials) .png ", plot = last_plot(), device = NULL, path = NULL,
  width = 12, height = 6, units = c("in", "cm", "mm"),
  dpi = 600)



afex.Iconicity.Early <- mixed(RespCorr ~ Iconicity + (1|ParticipantID),
                         data=OmnibusEarly4, family = binomial,
                         method = 'LRT')


#Pretty up results and kick out as kable
afex.Iconicity.Early$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Early Trials by Iconicity - ANOVA tables", ) %>%
              kable_styling(full_width= F)


summary(afex.Iconicity.Early)$coefficients  %>%
  data.frame() %>%
    tibble::rownames_to_column(var= "Iconicity") %>% 
    mutate(Iconicity = recode_factor(Iconicity, 
                                       `Iconicity1` = "Iconic)",
                                        `Iconicity2` = "Counter-Iconic",
                                       `Iconicity3` = "Conventional",
                                                                           )) %>%
      `colnames<-`(c("Iconicity", "Estimate", "SE", "z", "p")) %>%
        mutate_if(is.numeric, round, digits = 3) %>% 
          mutate_at("p", funs(ifelse(.<0.001, "<0.001", .))) %>%
            knitr::kable(caption = "Post Hoc Comparisons of Levels of Iconicity in Earliest Trials", ) %>%
              kable_styling(full_width= F) 


```


There is always a fine balance here - we want to avoid "p hacking", but there is a tension between the number of trials we'd expect to see an early advantage on, and having an actual sufficient amount of data to make a statistical comparison

For posterity, we'll run this comparison again using just the first 2 trials (highest chance, lowest data) and 8 trials (lower chance, but more data)


```{r Early Trials Analysis II}
#First 2 Trials
OmnibusEarly2 <- subset(Exp4Data, TrialNum < 3)

OmnibusEarly2Agg2 <- summarySE(OmnibusEarly2, measurevar = "RespCorr", groupvars = c("Iconicity"))

#Plot our data
ggplot(data = OmnibusEarly2Agg2, aes(x= Iconicity, y= RespCorr)) +
  geom_bar(stat = "summary", fun.y = "mean", aes(fill = Iconicity), position = position_dodge(width=0.9)) +
  geom_errorbar(aes(ymin= RespCorr - ci, ymax= RespCorr + ci, color= Iconicity), width= 0.2, size = 1, position=pd)+
  #scale_color_manual(values= c("#000000", "#007e70","#00b19d", "#00e4ca", "#c144c1","#8b2e8b", "#652165", "gray")) +
  #scale_linetype_manual(values = c("dashed", rep("solid", 3), rep("dashed",4))) +
  labs(x="Iconicity", y="Proportion of Correct Responses") +
  #scale_y_continuous(limits = c(0.40,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  theme_alan() +
  ggtitle("Early Trial Analysis - First 2 Trials by Iconicity")

#GGSAVE
ggsave("C:/Users/Alank/Documents/GitHub/Stekic-et-al/Figures/Early Trial Analysis by Iconicity (2 Trials).png", plot = last_plot(), device = NULL, path = NULL,
  width = 12, height = 6, units = c("in", "cm", "mm"),
  dpi = 600)



afex.Iconicity.Early2 <- mixed(RespCorr ~ Iconicity + (1|ParticipantID),
                         data=OmnibusEarly2, family = binomial,
                         method = 'LRT')


#Pretty up results and kick out as kable
afex.Iconicity.Early2$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Early Trials by Iconicity (2)- ANOVA tables", ) %>%
              kable_styling(full_width= F)

```

Nothing there. HOw about the first 8

```{r Early Trials Analysis III}
#First 8 Trials
OmnibusEarly8 <- subset(Exp4Data, TrialNum < 9)

OmnibusEarly8Agg2 <- summarySE(OmnibusEarly8, measurevar = "RespCorr", groupvars = c("Iconicity"))

#Plot our data
ggplot(data = OmnibusEarly8Agg2, aes(x= Iconicity, y= RespCorr)) +
  geom_bar(stat = "summary", fun.y = "mean", aes(fill = Iconicity), position = position_dodge(width=0.9)) +
  geom_errorbar(aes(ymin= RespCorr - ci, ymax= RespCorr + ci, color= Iconicity), width= 0.2, size = 1, position=pd)+
  #scale_color_manual(values= c("#000000", "#007e70","#00b19d", "#00e4ca", "#c144c1","#8b2e8b", "#652165", "gray")) +
  #scale_linetype_manual(values = c("dashed", rep("solid", 3), rep("dashed",4))) +
  labs(x="Iconicity", y="Proportion of Correct Responses") +
  #scale_y_continuous(limits = c(0.40,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  theme_alan() +
  ggtitle("Early Trial Analysis - First 8 Trials by Iconicity")

#GGSAVE
ggsave("C:/Users/Alank/Documents/GitHub/Stekic-et-al/Figures/Early Trial Analysis by Iconicity (8 Trials).png", plot = last_plot(), device = NULL, path = NULL,
  width = 12, height = 6, units = c("in", "cm", "mm"),
  dpi = 600)



afex.Iconicity.Early8 <- mixed(RespCorr ~ Iconicity + (1|ParticipantID),
                         data=OmnibusEarly8, family = binomial,
                         method = 'LRT')


#Pretty up results and kick out as kable
afex.Iconicity.Early8$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Early Trials by Iconicity (8)- ANOVA tables", ) %>%
              kable_styling(full_width= F)

```


It looks like we start showing an effect here, but I think it's just the main effect of No Label being shit in the Training trials coming up. There isn't much of a reason to look further into it, other than exploring for the potential intereference effect for Counter-Iconic Trials (overall)

Recall that in both my thesis (Nielsen, 2016) and some of Vanja's earlier work there is an interference effect for Counter-Iconic languages. They are learned just as well as Iconic ones, but participants are much slower on them, as if they are intentionally and consciously being forced to reverse their expectations

So we'll quickly take a look at that

```{r Exploration of Interference Effect}

Interference.RT <- summarySE(Exp4Data, measurevar = "RT", groupvars = c("Iconicity"))

#Plot our data
ggplot(data = Interference.RT, aes(x= Iconicity, y= RT)) +
  geom_bar(stat = "summary", fun.y = "mean", aes(fill = Iconicity), position = position_dodge(width=0.9)) +
  geom_errorbar(aes(ymin= RT - ci, ymax= RT + ci, color= Iconicity), width= 0.2, size = 1, position=pd)+
  #scale_color_manual(values= c("#000000", "#007e70","#00b19d", "#00e4ca", "#c144c1","#8b2e8b", "#652165", "gray")) +
  #scale_linetype_manual(values = c("dashed", rep("solid", 3), rep("dashed",4))) +
  labs(x="Iconicity", y="Response Time") +
  #scale_y_continuous(limits = c(0.40,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  theme_alan() +
  ggtitle("Response Time Interference Effect")

```

Alan Nielsen, the Patron Saint of Null Results, Strikes again!
Lets look quickly in early trials (and then cry)


```{r Interference Effects - Training Trials}
Interference.RT2 <- summarySE(subset(Exp4Data, TrialType == "Training"), measurevar = "RT", groupvars = c("Iconicity"))

#Plot our data
ggplot(data = Interference.RT2, aes(x= Iconicity, y= RT)) +
  geom_bar(stat = "summary", fun.y = "mean", aes(fill = Iconicity), position = position_dodge(width=0.9)) +
  geom_errorbar(aes(ymin= RT - ci, ymax= RT + ci, color= Iconicity), width= 0.2, size = 1, position=pd)+
  #scale_color_manual(values= c("#000000", "#007e70","#00b19d", "#00e4ca", "#c144c1","#8b2e8b", "#652165", "gray")) +
  #scale_linetype_manual(values = c("dashed", rep("solid", 3), rep("dashed",4))) +
  labs(x="Iconicity", y="Response Time") +
  #scale_y_continuous(limits = c(0.40,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  theme_alan() +
  ggtitle("Response Time Interference Effect (Training Only)")


#And just for shits and giggles, only in the first block

Interference.RT3 <- summarySE(subset(Part4Data, TrialType == "Training" & Block == 1), measurevar = "RT", groupvars = c("Iconicity"))

#Plot our data
ggplot(data = Interference.RT3, aes(x= Iconicity, y= RT)) +
  geom_bar(stat = "summary", fun.y = "mean", aes(fill = Iconicity), position = position_dodge(width=0.9)) +
  geom_errorbar(aes(ymin= RT - ci, ymax= RT + ci, color= Iconicity), width= 0.2, size = 1, position=pd)+
  #scale_color_manual(values= c("#000000", "#007e70","#00b19d", "#00e4ca", "#c144c1","#8b2e8b", "#652165", "gray")) +
  #scale_linetype_manual(values = c("dashed", rep("solid", 3), rep("dashed",4))) +
  labs(x="Iconicity", y="Response Time") +
  #scale_y_continuous(limits = c(0.40,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  theme_alan() +
  ggtitle("Response Time Interference Effect (First Block Only)")


```


There are a few more things we can do, but before that we're going to look at collapsing our data further so we have a better chance of pulling out some of the interesting effects

## Collapsing within Clusters for Analysis

We know that when we look at the omnibus results we want to be able to make some pretty general points - hence we collapse subconditions and all arbitrary conditions after looking at them

However, we leave our clusters (i.e. All Category labels) together, despite the fact that we know from our statistical tests that they aren't any different from each other

So lets collapse them for some nice graphs (and to simplify some of the below fiddly analyses)

```{r Parsimonious graphs and stats}


CleanData.Parsimonious <- CleanData.Final
CleanData.Parsimonious$Condition <- factor(CleanData.Parsimonious$Condition)

CleanData.Parsimonious$Condition2 <- factor(mapvalues(CleanData.Parsimonious$Condition,
                                 from = as.vector(levels(CleanData.Parsimonious$Condition)),
                                 to = c("Category", "No Label", rep("Category",3), rep("Item", 4), rep("Arbitrary", 3))
                                 ))


```


## Testing Stimuli

In this study, as in Lupyan et al. (2007), we have two kinds of stimuli presented in Testing Trials - either images that were seen previously, or images of the same type, but unseen (as a test of generalisation). 

However, nowhere in Lupyan et al (2007) is the difference between these types of trials reported (which suggests to me that there we no differences)

Currently we show this split in all of our graphs, but why we do so isn't particularly well motivated. It would certianly be easier for us to show only one Testing set in graphs and also to collapse this for statistical analyses, we lets take a look

```{r Justifying Testing Type Collapse}

TypeCollapse <- subset(Part4Data, TrialType2 != "Training")
TypeCollapse$TrialType2 <- factor(TypeCollapse$TrialType2)
afex.TypeCollapse <- mixed(RespCorr ~ Condition * TrialType2 + (1|ParticipantID),
                         data=TypeCollapse,
                         family=binomial,
                         control=glmerControl(optimizer="bobyqa"),
                         method = 'LRT')

#Pretty up results and kick out as kable
afex.TypeCollapse$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Mixed Model Results - Part 2", ) %>%
              kable_styling(full_width= F)



TypeCollapseAgg <- summarySE(TypeCollapse, measurevar = "RespCorr", groupvars = c("Condition", "TrialType2", "Block"))

#Plot our data
ggplot(data=TypeCollapseAgg, aes(x=Block, y=RespCorr, group= Condition)) +
  geom_line(aes(linetype = Condition, color = Condition), size = 1.2, position=pd) +
  geom_errorbar(aes(ymin= RespCorr - ci, ymax= RespCorr + ci, color= Condition), width= 0.2, size = 1, position=pd)+
  scale_color_manual(values= c("#000000", "#007e70","#00b19d", "#00e4ca", "#c144c1","#8b2e8b", "#652165", "gray")) +
  scale_linetype_manual(values = c("dashed", rep("solid", 3), rep("dashed",4))) +
  labs(x="Block", y="Proportion of Correct Responses") +
  scale_y_continuous(limits = c(0.40,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  facet_grid(~TrialType2, scales="free", space= "free_x") +
  theme_alan() +
  ggtitle("Comparison of Testing Types")



#INTERACTIONS
TypeCollapse.vPairs <- data.frame(pairs(emmeans(afex.TypeCollapse, 
                    ~Condition * TrialType2,
                    adjust = "none")))

#Comparisons 
TypeCollapse.EMMeansTable <-   rbind(TypeCollapse.vPairs[8,], #No Label
                                     TypeCollapse.vPairs[23,], #Icxonic (Category)
                                     TypeCollapse.vPairs[37,], #Counter-Iconic (Category)
                                     TypeCollapse.vPairs[50,], #Conventional (Category)
                                     TypeCollapse.vPairs[62,], #Iconic (Item)
                                     TypeCollapse.vPairs[73,], #Counter-Iconic (Item)
                                     TypeCollapse.vPairs[83,], #Conventional (Item)
                                     TypeCollapse.vPairs[92,] #Arbitrary
)



#COMPARISONS OF CONDITIONS WITHIN TRAINING TRIALS
TypeCollapse.Table <- cbind.data.frame(
  levels(TypeCollapse$Condition), #Conditions
  as.vector(tapply(subset(TypeCollapse, TrialType2 == "Testing-Old")$RespCorr, 
                   subset(TypeCollapse, TrialType2 == "Testing-Old")$Condition, mean)), #oldmeans
  as.vector(tapply(subset(TypeCollapse, TrialType2 == "Testing-New")$RespCorr, 
                   subset(TypeCollapse, TrialType2 == "Testing-New")$Condition, mean)),  #NewMeans
  TypeCollapse.EMMeansTable
)

TypeCollapse.Table %>% 
    `colnames<-`(c("Condition", "Mean (Old Items)", "Mean (New Items)", "Contrast", "Estimate", "SE", "df", "z Ratio", "p")) %>%
    mutate_if(is.numeric, round, 3) %>%
    mutate_at("p", funs(ifelse(.<0.001, "<0.001", .))) %>%
    subset(select = -c(Contrast, df)) %>%
    subset(select = c(Condition, `Mean (Old Items)`, `Mean (New Items)`, p, Estimate, SE, `z Ratio`))%>%
          knitr::kable(caption = 'Post Hoc Comparisons of Condition between Testing Types', "html") %>%
            kable_styling(full_width = F)  



```



Hip hip to the hooray - this is exactly what we expected- even though there is a significant main effect of Trial TYpe and a significant Interaction, this post hoc comparison (which we are licensed to make) tells us the only really thing we are interested in: every condition other than the No Label condition has equal performance on both kinds of items, but in the No Label condition performance in the testing trials is better on Old (familiar) items


## Subconditions

OUr experimental participants are split into two subconditions giving informattion about which kind of "alien microbes" they should approach - whether they should approach Curvy ones or Jagged ones

We suggested in our OSF that we expect it will be easier to learn to approach Curvy aliens, regardless of labelling condition, but would ideally hope that this wasn't a very large effect. Nonetheless, it is one that we should look at

To do this, we're going to use the collapsed "parsimonious" data from above

```{r SubCOndition Exploration}
CleanData.Parsimonious$Subcondition <- factor(CleanData.Parsimonious$Subcondition)

afex.Parsim.SubCond <- mixed(RespCorr ~ Condition2 * Subcondition + (1|ParticipantID),
                         data=CleanData.Parsimonious,
                         family=binomial,
                         control=glmerControl(optimizer="bobyqa"),
                         method = 'LRT')

#Pretty up results and kick out as kable
afex.Parsim.SubCond$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Subcondition Analysis", ) %>%
              kable_styling(full_width= F)


SubCondition.Parsim.Agg <- summarySE(CleanData.Parsimonious, measurevar = "RespCorr", groupvars = c("Condition2", "Subcondition"))

#Plot our data
ggplot(data = SubCondition.Parsim.Agg, aes(x= Subcondition, y= RespCorr)) +
  geom_bar(stat = "summary", fun.y = "mean", aes(fill = Subcondition), position = position_dodge(width=0.9)) +
  geom_errorbar(aes(ymin= RespCorr - ci, ymax= RespCorr + ci, color= Subcondition), width= 0.2, size = 1, position=pd)+
  #scale_color_manual(values= c("#000000", "#007e70","#00b19d", "#00e4ca", "#c144c1","#8b2e8b", "#652165", "gray")) +
  #scale_linetype_manual(values = c("dashed", rep("solid", 3), rep("dashed",4))) +
  labs(x="Iconicity", y="Response Time") +
  #scale_y_continuous(limits = c(0.40,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  theme_alan() +
  ggtitle("Comparison of Subconditions within Parsimonious Conditions") +
    facet_grid(~Condition2)


```


So we can see no interaction of effects here, just a benefit in accuracy when the rules for Approach are that the explorers should approach Curvy Aliens

Is this something that persists across the entire experiment, or is it an artifact of early trials (i.e. is this swamping the normal early trial benefit that we'd have seen in other studies for iconicity). Lets take a look just within the first block of training

```{r Early Approach}

CleanData.Parsimonious.Early <- subset(CleanData.Parsimonious, TrialType == "Training" & Block == 1)

afex.Parsim.Early.SubCond <- mixed(RespCorr ~ Condition2 * Subcondition + (1|ParticipantID),
                         data=CleanData.Parsimonious.Early,
                         family=binomial,
                         control=glmerControl(optimizer="bobyqa"),
                         method = 'LRT')

#Pretty up results and kick out as kable
afex.Parsim.Early.SubCond$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Subcondition Analysis (First Block)", ) %>%
              kable_styling(full_width= F)


SubCondition.Parsim.Early.Agg <- summarySE(CleanData.Parsimonious.Early, measurevar = "RespCorr", groupvars = c("Condition2", "Subcondition"))

#Plot our data
ggplot(data = SubCondition.Parsim.Early.Agg, aes(x= Subcondition, y= RespCorr)) +
  geom_bar(stat = "summary", fun.y = "mean", aes(fill = Subcondition), position = position_dodge(width=0.9)) +
  geom_errorbar(aes(ymin= RespCorr - ci, ymax= RespCorr + ci, color= Subcondition), width= 0.2, size = 1, position=pd)+
  #scale_color_manual(values= c("#000000", "#007e70","#00b19d", "#00e4ca", "#c144c1","#8b2e8b", "#652165", "gray")) +
  #scale_linetype_manual(values = c("dashed", rep("solid", 3), rep("dashed",4))) +
  labs(x="Iconicity", y="Response Time") +
  #scale_y_continuous(limits = c(0.40,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  theme_alan() +
  ggtitle("Comparison of Subconditions within Parsimonious Conditions (First Block Only)") +
    facet_grid(~Condition2)



```

Not a *ton* going on here, but definitely some suggestion that for the No Label condition participants might start with a large bias towards approaching Curvy Aliens and then have this bias go away over time (differences in the whole Experiment are much smaller/might not exist in that condition). This could be explored further later if we want to, but certainly isn't worth it at this juncture

But speaking of useless things not worth the time to do them - lets make an extra extra prediction - everyone will do better on Testing-New trials when this rule is congruent (because they will have nothing to fall back on)

```{r Subconditions and New Item Testing Trials}

CleanData.Parsimonious.NewTest <- subset(CleanData.Parsimonious, TrialType2 == "Testing-New" & Block == 1)

afex.Parsim.NewTest.SubCond <- mixed(RespCorr ~ Condition2 * Subcondition + (1|ParticipantID),
                         data=CleanData.Parsimonious.NewTest,
                         family=binomial,
                         control=glmerControl(optimizer="bobyqa"),
                         method = 'LRT')

#Pretty up results and kick out as kable
afex.Parsim.NewTest.SubCond$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Subcondition Analysis (First Block of New Test Items)", ) %>%
              kable_styling(full_width= F)


CleanData.Parsimonious.NewTest.Agg <- summarySE(CleanData.Parsimonious.NewTest, measurevar = "RespCorr", groupvars = c("Condition2", "Subcondition"))

#Plot our data
ggplot(data = CleanData.Parsimonious.NewTest.Agg, aes(x= Subcondition, y= RespCorr)) +
  geom_bar(stat = "summary", fun.y = "mean", aes(fill = Subcondition), position = position_dodge(width=0.9)) +
  geom_errorbar(aes(ymin= RespCorr - ci, ymax= RespCorr + ci, color= Subcondition), width= 0.2, size = 1, position=pd)+
  #scale_color_manual(values= c("#000000", "#007e70","#00b19d", "#00e4ca", "#c144c1","#8b2e8b", "#652165", "gray")) +
  #scale_linetype_manual(values = c("dashed", rep("solid", 3), rep("dashed",4))) +
  labs(x="Iconicity", y="Response Time") +
  #scale_y_continuous(limits = c(0.40,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  theme_alan() +
  ggtitle("Comparison of Subconditions within Parsimonious Conditions (First Block of Testing-New)") +
    facet_grid(~Condition2)




```


Sometimes it feels good to be wrong #patronsaintofnullresults


## COMPARISON OF BLOCKS 1, 6 , 9 in TRAINING TRIALS

```{r Learning Rates and Comparison of Trials}
Blocks.Data.Training <- subset(Part4Data, TrialType2 == "Training")
Blocks.Data.Training <- subset(Blocks.Data.Training, Block == 1|Block == 6|Block == 9)
Blocks.Data.Training$Block <- as.factor(Blocks.Data.Training$Block)

afex.Blocks <- mixed(RespCorr ~ Condition * Block + (1|ParticipantID),
                         data=Blocks.Data.Training,
                         family=binomial,
                         control=glmerControl(optimizer="bobyqa"),
                         method = 'LRT')

#Pretty up results and kick out as kable
afex.Blocks$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Mixed Model Results - Interaction of Condition x Block (Training Trials)", ) %>%
              kable_styling(full_width= F)

# POST HOC COMPARISONS (FOR TABLES)

BlocksTraining.vPairs <- data.frame(pairs(emmeans(afex.Blocks, 
                    ~Condition * Block,
                    adjust = "none")))


BlocksTraining.EMMeansTable.1v6 <-   rbind(BlocksTraining.vPairs[8,], #No Label
                                     BlocksTraining.vPairs[31,], #Icxonic (Category)
                                     BlocksTraining.vPairs[53,], #Counter-Iconic (Category)
                                     BlocksTraining.vPairs[74,], #Conventional (Category)
                                     BlocksTraining.vPairs[94,], #Iconic (Item)
                                     BlocksTraining.vPairs[113,], #Counter-Iconic (Item)
                                     BlocksTraining.vPairs[131,], #Conventional (Item)
                                     BlocksTraining.vPairs[148,] #Arbitrary
)

BlocksTraining.EMMeansTable.1v9 <-   rbind(BlocksTraining.vPairs[16,], #No Label
                                     BlocksTraining.vPairs[39,], #Icxonic (Category)
                                     BlocksTraining.vPairs[61,], #Counter-Iconic (Category)
                                     BlocksTraining.vPairs[82,], #Conventional (Category)
                                     BlocksTraining.vPairs[102,], #Iconic (Item)
                                     BlocksTraining.vPairs[121,], #Counter-Iconic (Item)
                                     BlocksTraining.vPairs[139,], #Conventional (Item)
                                     BlocksTraining.vPairs[156,] #Arbitrary
)

BlocksTraining.EMMeansTable.6v9 <-   rbind(BlocksTraining.vPairs[164,], #No Label
                                     BlocksTraining.vPairs[179,], #Icxonic (Category)
                                     BlocksTraining.vPairs[193,], #Counter-Iconic (Category)
                                     BlocksTraining.vPairs[206,], #Conventional (Category)
                                     BlocksTraining.vPairs[218,], #Iconic (Item)
                                     BlocksTraining.vPairs[229,], #Counter-Iconic (Item)
                                     BlocksTraining.vPairs[239,], #Conventional (Item)
                                     BlocksTraining.vPairs[248,] #Arbitrary
)

#Comparisons of Blocks 1, 6, and 9 Within Conditions
BlocksTraining.Table <- cbind.data.frame(
  levels(Blocks.Data.Training$Condition), #Conditions
  as.vector(tapply(subset(Blocks.Data.Training, Block == "1")$RespCorr, 
                   subset(Blocks.Data.Training, Block == "1")$Condition, mean)), #Block 1 Means
  
    as.vector(tapply(subset(Blocks.Data.Training, Block == "6")$RespCorr, 
                   subset(Blocks.Data.Training, Block == "6")$Condition, mean)), #Block 6 Means
  
    as.vector(tapply(subset(Blocks.Data.Training, Block == "9")$RespCorr, 
                   subset(Blocks.Data.Training, Block == "9")$Condition, mean)), #Block 9 Means
  
  BlocksTraining.EMMeansTable.1v6, #1 vs 6 comparisons
  BlocksTraining.EMMeansTable.1v9,# 1 vs 9 comparisons
  BlocksTraining.EMMeansTable.6v9 #6 vs 9 comparisons
  )

BlocksTraining.Table %>% 
    `colnames<-`(c("Condition", "Block 1", "Block 2", "Block 3", 
                   "Contrast (1v6)", "Estimate (1v6)", "SE (1v6)", "df (1v6)", "z Ratio (1v6)", "p (1v6)",
                   "Contrast (1v9)", "Estimate (1v9)", "SE (1v9)", "df (1v9)", "z Ratio (1v9)", "p (1v9)",
                   "Contrast (6v9)", "Estimate (6v9)", "SE (6v9)", "df (6v9)", "z Ratio (6v9)", "p (6v9)"
                   )) %>%
    mutate_if(is.numeric, round, 3) %>%
    mutate_at(.vars = vars("p (1v6)", "p (1v9)", "p (6v9)"), funs(ifelse(.<0.001, "<0.001", .))) %>%
    subset(select = -c(`Contrast (1v6)`, `df (1v6)`, `Estimate (1v6)`,
                       `Contrast (1v9)`, `df (1v9)`, `Estimate (1v9)`,
                       `Contrast (6v9)`, `df (6v9)`, `Estimate (6v9)`
                       )) %>%
          knitr::kable(caption = 'Post Hoc Comparisons of Condition between Blocks 1, 6, and 9', "html") %>%
            kable_styling(full_width = F)  %>%
                add_header_above(c("", "Means by Block" = 3, 
                               "Block 1 vs Block 6" = 3,
                                "Block 1 vs Block 9" = 3,
                               "Block 6 vs Block 9" = 3),
                               font_size = 12)


##COLLAPSE AND PLOT
Blocks.Data.TrainingAgg <- summarySE(Blocks.Data.Training, measurevar = "RespCorr", groupvars = c("Condition", "Block"))

#Plot our data
ggplot(data=Blocks.Data.TrainingAgg, aes(x=Block, y=RespCorr, group= Condition)) +
  geom_line(aes(linetype = Condition, color = Condition), size = 1.2, position=pd) +
  geom_errorbar(aes(ymin= RespCorr - ci, ymax= RespCorr + ci, color= Condition), width= 0.2, size = 1, position=pd)+
  scale_color_manual(values= c("#000000", "#007e70","#00b19d", "#00e4ca", "#c144c1","#8b2e8b", "#652165", "gray")) +
  scale_linetype_manual(values = c("dashed", rep("solid", 3), rep("dashed",4))) +
  labs(x="Block", y="Proportion of Correct Responses") +
  scale_y_continuous(limits = c(0.40,1), breaks=c(0.5,0.6,0.7,0.8,0.9,1.0)) +
  theme_alan() +
  ggtitle("Comparison of Blocks 1, 6, and 9 by Condition")

```

That's pretty much what we expected/wanted. Everyone gets better between Blocks 1 and 6, only 2 conditions (counter iconic (item) and conventional (item) continue to get better between block 6 and 9)



## Comparisons of Blocks within Testing Trials

```{r Comparison of blocks within testing trials}

Blocks.Data.Testing <- subset(Part4Data, TrialType == "Testing")
Blocks.Data.Testing$Block <- as.factor(Blocks.Data.Testing$Block)

afex.Blocks.Testing <- mixed(RespCorr ~ Condition * Block + (1|ParticipantID),
                         data=Blocks.Data.Testing,
                         family=binomial,
                         control=glmerControl(optimizer="bobyqa"),
                         method = 'LRT')

#Pretty up results and kick out as kable
afex.Blocks.Testing$anova_table %>% 
  data.frame() %>% 
    tibble::rownames_to_column(var= "Variable") %>% 
      mutate_if(is.numeric, round, digits = 3) %>% 
        mutate_all(funs(ifelse(.<0.001, "<0.001", .))) %>%
          `colnames<-`(c("Variable", "df", "Chi Sq", "Chi df", "p")) %>%
            knitr::kable(caption = "Mixed Model Results - Interaction of Condition x Block (Testing Trials)", ) %>%
              kable_styling(full_width= F)

```

Excellent - there is no difference between Testing BLocks by Conditino, so we are justified in cutting down on the number of testing blocks without any real argument

              
              
## STUDY LENGTH JUSTIFICATION

```{r Study Length Exploration}

#Find the average length of the study (Trials) per participant
MeanTrial <- mean(subset(CleanData.Final, Condition == "4"|Condition == "5"|Condition == "6A"|Condition == "6B")$RT) #Mean Trial Length
TotalTrials <- MeanTrial*240/1000 #Total Length in Seconds
TotalTrialsMin <- TotalTrials/60

TestLength <- cbind.data.frame(c("Mean Trial Length", "Total Length (s)", "Total Length (M)"), c(MeanTrial, TotalTrials, TotalTrialsMin) )

# Current Length
TestLength %>% 
           `colnames<-`(c("", "Current")) %>%
            knitr::kable(caption = "Experiment Length", ) %>%
              kable_styling(full_width= F)



#Tables of Options

OptionsTable <- cbind.data.frame(c("Trials","Length (s)", "Length (M)"),
                                 c(240, MeanTrial/1000*240, MeanTrial/1000*240/60), #Current Experiment
                                 c(192, MeanTrial/1000*192, MeanTrial/1000*192/60), #Same Number of test stims (4 Blocks)
                                 c(168, MeanTrial/1000*168, MeanTrial/1000*168/60), #Same Number of test stims (3 Blocks)
                                 c(144, MeanTrial/1000*144, MeanTrial/1000*144/60), #Same Number of test stims (2 Blocks)
                                 c(160, MeanTrial/1000*160, MeanTrial/1000*160/60), #Less test stims (4 Blocks)
                                 c(144, MeanTrial/1000*144, MeanTrial/1000*144/60), #Less test stims (3 Blocks)
                                 c(128, MeanTrial/1000*128, MeanTrial/1000*128/60) #Less test stims (2 Blocks)
  )

OptionsTable %>% 
           `colnames<-`(c(" ", "Pilot", 
                          "6/4", "6/3", "6/2",
                          "6/4 (S)", "6/3 (S)", "6/2 (S)")) %>%
        mutate_if(is.numeric, round, digits = 0) %>%
            knitr::kable(caption = "Potential Experiment Lengths (Not Including Instructions)", ) %>%
              kable_styling(full_width= F) %>%
                add_header_above(c(" " = 2, "6 Training Blocks + Normal Testing Blocks" = 3, 
                               "6 Training Blocks + Short Testing Blocks" = 3),
                               font_size = 12)


```






